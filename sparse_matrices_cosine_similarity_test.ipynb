{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fnmatch\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from preprocessing import *\n",
    "from affichage import *\n",
    "from swSets import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lost, Heroes, Jericho, Prison Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la liste:\n",
    "- chaque dictionnaire represente une serie du corpus\n",
    "\n",
    "Dans les dictionnaires:\n",
    "- les cles sont les numeros des saisons\n",
    "- les valeurs sont l'indice max (indMax-1 en fait) des episodes de la saisons dans le dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 24, 2: 48, 3: 72, 4: 85, 5: 105, 6: 125},\n",
       " {1: 24, 2: 35, 3: 61, 4: 80},\n",
       " {1: 22, 2: 29},\n",
       " {1: 22, 2: 44, 3: 57, 4: 81}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_dict_list = getShowDictList(path)\n",
    "show_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition de l'ensemble de stopwords\n",
    "nltk_sw = set(stopwords.words('english'))\n",
    "sklearn_sw = set(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopwords_set = nltk_sw | sklearn_sw | english_contractions_tokens |sw\n",
    "l_nb = [str(i) for i in range(1000000)]\n",
    "l_mots = [\"don\", \"yeah\", \"hey\", \"okay\", \"oh\", \"uh\", \"yes\", \"ok\"]\n",
    "for mot in l_mots :\n",
    "    stopwords_set.add(mot)\n",
    "for nb in l_nb:\n",
    "    stopwords_set.add(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = []\n",
    "for show_dict in show_dict_list:\n",
    "    show_tfidf_vect_dict = dict()\n",
    "    minInd = 0\n",
    "    for season, maxInd in show_dict.items():\n",
    "        show_tfidf_vect_dict[season] = scipy.sparse.csr_matrix(tfidf_df[minInd:maxInd].values)\n",
    "        minInd= maxInd\n",
    "    dict_list.append(show_tfidf_vect_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la liste:\n",
    "    \n",
    "- chaque dictionnaire represente une serie du corpus\n",
    "\n",
    "Dans les dictionnaires:\n",
    "\n",
    "- les clefs sont les numeros des saisons\n",
    "- les valeurs sont des matrices sparses, dont les lignes sont les vecteurs tf-idf des episodes de la saisons donnée par la clef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: <24x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 10625 stored elements in Compressed Sparse Row format>,\n",
       "  2: <24x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 9610 stored elements in Compressed Sparse Row format>,\n",
       "  3: <24x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 10412 stored elements in Compressed Sparse Row format>,\n",
       "  4: <13x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 6376 stored elements in Compressed Sparse Row format>,\n",
       "  5: <20x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 10827 stored elements in Compressed Sparse Row format>,\n",
       "  6: <20x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 9333 stored elements in Compressed Sparse Row format>},\n",
       " {1: <24x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 10625 stored elements in Compressed Sparse Row format>,\n",
       "  2: <11x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 3812 stored elements in Compressed Sparse Row format>,\n",
       "  3: <26x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 11244 stored elements in Compressed Sparse Row format>,\n",
       "  4: <19x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 9179 stored elements in Compressed Sparse Row format>},\n",
       " {1: <22x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 9529 stored elements in Compressed Sparse Row format>,\n",
       "  2: <7x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 2870 stored elements in Compressed Sparse Row format>},\n",
       " {1: <22x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 9529 stored elements in Compressed Sparse Row format>,\n",
       "  2: <22x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 8604 stored elements in Compressed Sparse Row format>,\n",
       "  3: <13x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 5695 stored elements in Compressed Sparse Row format>,\n",
       "  4: <24x19932 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 11483 stored elements in Compressed Sparse Row format>}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarite cosinus entre les episodes de la saisons 1 de Prison Break\n",
    "#similarities_sparse = cosine_similarity(dict_list[3][1] ,dense_output=False)\n",
    "#print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarité cosinus entre NCIS and NCIS Los Angeles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hello i'd like to report encoding problems\n",
    "\n",
    "\n",
    "<br>\n",
    "Les deux NCIS là: \"ISO-8859-1\" ---> en fait BOF\n",
    "<br>\n",
    "Lost : \"utf-8\"\n",
    "<br>\n",
    "bref à voir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbSeriesCorpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 23,\n",
       "  2: 46,\n",
       "  3: 70,\n",
       "  4: 94,\n",
       "  5: 113,\n",
       "  6: 138,\n",
       "  7: 162,\n",
       "  8: 186,\n",
       "  9: 210,\n",
       "  10: 234,\n",
       "  11: 258,\n",
       "  12: 282,\n",
       "  13: 288},\n",
       " {1: 24, 2: 48, 3: 72, 4: 96, 5: 120, 6: 144, 7: 149}]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_dict_list = getShowDictList(path)\n",
    "show_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIS_texts = list(corpus[:288])\n",
    "LA_texts = list(corpus[288:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "NCIS_str = \"\"\n",
    "for ep in NCIS_texts:\n",
    "    NCIS_str += ep+\" \"\n",
    "c.append(NCIS_str)\n",
    "LA_str = \"\"\n",
    "for ep in LA_texts:\n",
    "       LA_str += ep+\" \"\n",
    "c.append(LA_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(c)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mat = scipy.sparse.csr_matrix(tfidf_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairwise sparse output:\n",
      "   (0, 1)\t0.687275815227\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 0)\t0.687275815227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarities_sparse = cosine_similarity(sparse_mat ,dense_output=False)\n",
    "print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La similarité cosinus entre NCIS et NCIS Los Angeles est donc : 0.687275815227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarité cosinus entre NCIS and Jane Eyre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data\"\n",
    "corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 23,\n",
       "  2: 46,\n",
       "  3: 70,\n",
       "  4: 94,\n",
       "  5: 113,\n",
       "  6: 138,\n",
       "  7: 162,\n",
       "  8: 186,\n",
       "  9: 210,\n",
       "  10: 234,\n",
       "  11: 258,\n",
       "  12: 282,\n",
       "  13: 288},\n",
       " {1: 4, 2: 6}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_dict_list = getShowDictList(path)\n",
    "show_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCIS_texts = list(corpus[:288])\n",
    "JA_texts = list(corpus[288:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []\n",
    "NCIS_str = \"\"\n",
    "for ep in NCIS_texts:\n",
    "    NCIS_str += ep+\" \"\n",
    "c.append(NCIS_str)\n",
    "JA_str = \"\"\n",
    "for ep in JA_texts:\n",
    "       JA_str += ep+\" \"\n",
    "c.append(JA_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(c)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairwise sparse output:\n",
      "   (0, 1)\t0.0762084598029\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (1, 0)\t0.0762084598029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparse_mat = scipy.sparse.csr_matrix(tfidf_df.values)\n",
    "similarities_sparse = cosine_similarity(sparse_mat ,dense_output=False)\n",
    "print('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La similarité cosinus entre NCIS et Jane Eyre est donc : 0.0762084598029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrouver la saison d'un episode avec la similarité cosinus : essai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"data/210___Spaced\"\n",
    "path = \"data\"\n",
    "corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 7, 2: 14}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_dict_list = getShowDictList(path)\n",
    "show_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_mat = scipy.sparse.csr_matrix(tfidf_df.values)\n",
    "similarities = cosine_similarity(sparse_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'season1': 0.61450216915888722, 'season2': 0.068669633059526419},\n",
       " 1: {'season1': 0.66640209083355495, 'season2': 0.045901230528520097},\n",
       " 2: {'season1': 0.58375941095513972, 'season2': 0.035742908287915993},\n",
       " 3: {'season1': 0.68701127712658605, 'season2': 0.049503253188658621},\n",
       " 4: {'season1': 0.58774854060249071, 'season2': 0.036926135631595171},\n",
       " 5: {'season1': 0.63798897406590027, 'season2': 0.054454027081937785},\n",
       " 6: {'season1': 0.64358048201676166, 'season2': 0.05188742348895424},\n",
       " 7: {'season1': 0.046686846398172541, 'season2': 0.27219075761329498},\n",
       " 8: {'season1': 0.05004753068206462, 'season2': 0.25505493265387436},\n",
       " 9: {'season1': 0.035675078859271965, 'season2': 0.20192026933672222},\n",
       " 10: {'season1': 0.045612893082043873, 'season2': 0.24451884566019702},\n",
       " 11: {'season1': 0.056904728989134901, 'season2': 0.27534523298248553},\n",
       " 12: {'season1': 0.041655459647107353, 'season2': 0.27644899410506618},\n",
       " 13: {'season1': 0.066502073609313109, 'season2': 0.29119544094879174}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_saisons = len(show_dict_list[0].keys())\n",
    "nb_eps = show_dict_list[0][nb_saisons]\n",
    "\n",
    "dico_mean_scores = dict()\n",
    "for i in range(nb_eps):\n",
    "    dico_mean_scores[i] = dict()\n",
    "    scores_season1 = []\n",
    "    scores_season2 = []\n",
    "    for j in range(nb_eps):\n",
    "        if i != j:\n",
    "            if j < 7:\n",
    "                scores_season1.append(similarities[i][j])\n",
    "            else:\n",
    "                scores_season2.append(similarities[i][j])\n",
    "    dico_mean_scores[i][\"season1\"] = np.mean(scores_season1)\n",
    "    dico_mean_scores[i][\"season2\"] = np.mean(scores_season2)\n",
    "            \n",
    "dico_mean_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'season1',\n",
       " 1: 'season1',\n",
       " 2: 'season1',\n",
       " 3: 'season1',\n",
       " 4: 'season1',\n",
       " 5: 'season1',\n",
       " 6: 'season1',\n",
       " 7: 'season2',\n",
       " 8: 'season2',\n",
       " 9: 'season2',\n",
       " 10: 'season2',\n",
       " 11: 'season2',\n",
       " 12: 'season2',\n",
       " 13: 'season2'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "dico_result = dict()\n",
    "for ep, dico in dico_mean_scores.items():\n",
    "        dico_result[ep] = max(dico.items(), key=operator.itemgetter(1))[0]\n",
    "dico_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultats: tous les episodes ont été classés dans la bonne saison !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
