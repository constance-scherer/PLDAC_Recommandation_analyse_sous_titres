{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from nltk.corpus import stopwords\n",
    "from preprocessing import *\n",
    "from affichage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/1___Lost\"\n",
    "corpus = get_corpus(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_contractions_tokens = {'get', 'must', 'this', 'wo', 'may', 'she', 'where', \"who'd\", 'who', \"shouldn't\", '(', 'which', \"'d\", 'I', 'so', 'what', \"'ll\", 'gim', 'be', 'go', \"'re\", ')', 's', 'o', 'should', 'something', 'we', \"couldn't\", \"whom'st'd\", 'na', 'me', \"'t\", \"mustn't\", 'shall', \"'m\", 'ought', 'sha', 'they', 'why', 'those', 'everyone', 'gon', \"e'er\", 'it', \"ne'er\", \"'cause\", 'ai', 'finna', \"'ve\", \"'s\", \"'\", \"o'er\", 'could', 'ca', 'might', 'someone', 'somebody', 'these', 'you', \"n't\", 'da', \"o'clock\", 'would', 'let', 'there', 'noun', \"we'd\", 'ol', 'that', 'ta', 'he', \"y'all\", 'dare', 'need', 'when', 'a', 'how', 'have', 'do'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = {'a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', 'don', 'yeah', 're', 'okay', 'hey', 'll', 've', 'oh', 'didn', 'ok', 'am', 'yes', 'doin'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition de l'ensemble de stopwords\n",
    "nltk_sw = set(stopwords.words('english'))\n",
    "sklearn_sw = set(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopwords_set = nltk_sw | sklearn_sw | english_contractions_tokens |sw\n",
    "l_nb = [str(i) for i in range(1000000)]\n",
    "l_mots = [\"don\", \"yeah\", \"hey\", \"okay\", \"oh\", \"uh\", \"yes\", \"ok\"]\n",
    "for mot in l_mots :\n",
    "    stopwords_set.add(mot)\n",
    "for nb in l_nb:\n",
    "    stopwords_set.add(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "nb_occ_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set, tokenizer=lemmatizing_tokenizer_v2)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_df.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "      <th>top6</th>\n",
       "      <th>top7</th>\n",
       "      <th>top8</th>\n",
       "      <th>top9</th>\n",
       "      <th>top10</th>\n",
       "      <th>top11</th>\n",
       "      <th>top12</th>\n",
       "      <th>top13</th>\n",
       "      <th>top14</th>\n",
       "      <th>top15</th>\n",
       "      <th>top16</th>\n",
       "      <th>top17</th>\n",
       "      <th>top18</th>\n",
       "      <th>top19</th>\n",
       "      <th>top20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>help</td>\n",
       "      <td>transceiver</td>\n",
       "      <td>pen</td>\n",
       "      <td>shaft</td>\n",
       "      <td>drive</td>\n",
       "      <td>charlie</td>\n",
       "      <td>cockpit</td>\n",
       "      <td>jack</td>\n",
       "      <td>rescue</td>\n",
       "      <td>beth</td>\n",
       "      <td>look</td>\n",
       "      <td>body</td>\n",
       "      <td>rip</td>\n",
       "      <td>nerve</td>\n",
       "      <td>drape</td>\n",
       "      <td>shout</td>\n",
       "      <td>sew</td>\n",
       "      <td>monkey</td>\n",
       "      <td>seatbelt</td>\n",
       "      <td>bathroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bear</td>\n",
       "      <td>battery</td>\n",
       "      <td>iteration</td>\n",
       "      <td>guy</td>\n",
       "      <td>radio</td>\n",
       "      <td>sir</td>\n",
       "      <td>gun</td>\n",
       "      <td>french</td>\n",
       "      <td>seatbelt</td>\n",
       "      <td>polar</td>\n",
       "      <td>fasten</td>\n",
       "      <td>checker</td>\n",
       "      <td>signal</td>\n",
       "      <td>body</td>\n",
       "      <td>bar</td>\n",
       "      <td>tell</td>\n",
       "      <td>walt</td>\n",
       "      <td>waste</td>\n",
       "      <td>shut</td>\n",
       "      <td>excuse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wash</td>\n",
       "      <td>annie</td>\n",
       "      <td>gun</td>\n",
       "      <td>tell</td>\n",
       "      <td>cryin</td>\n",
       "      <td>dinosaur</td>\n",
       "      <td>lord</td>\n",
       "      <td>look</td>\n",
       "      <td>kate</td>\n",
       "      <td>dog</td>\n",
       "      <td>dangerous</td>\n",
       "      <td>mortgage</td>\n",
       "      <td>walk</td>\n",
       "      <td>signal</td>\n",
       "      <td>fever</td>\n",
       "      <td>damn</td>\n",
       "      <td>water</td>\n",
       "      <td>die</td>\n",
       "      <td>organize</td>\n",
       "      <td>picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>helen</td>\n",
       "      <td>boar</td>\n",
       "      <td>locke</td>\n",
       "      <td>tell</td>\n",
       "      <td>walkabout</td>\n",
       "      <td>fish</td>\n",
       "      <td>john</td>\n",
       "      <td>talk</td>\n",
       "      <td>rise</td>\n",
       "      <td>norman</td>\n",
       "      <td>randy</td>\n",
       "      <td>destiny</td>\n",
       "      <td>colonel</td>\n",
       "      <td>look</td>\n",
       "      <td>food</td>\n",
       "      <td>croucher</td>\n",
       "      <td>climb</td>\n",
       "      <td>sorry</td>\n",
       "      <td>hunt</td>\n",
       "      <td>fuselage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>water</td>\n",
       "      <td>jack</td>\n",
       "      <td>rent</td>\n",
       "      <td>swim</td>\n",
       "      <td>father</td>\n",
       "      <td>claire</td>\n",
       "      <td>car</td>\n",
       "      <td>look</td>\n",
       "      <td>jump</td>\n",
       "      <td>latitude</td>\n",
       "      <td>talk</td>\n",
       "      <td>decide</td>\n",
       "      <td>crazy</td>\n",
       "      <td>fail</td>\n",
       "      <td>chase</td>\n",
       "      <td>chrissy</td>\n",
       "      <td>wonderland</td>\n",
       "      <td>silverman</td>\n",
       "      <td>ugly</td>\n",
       "      <td>hairbrush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>people</td>\n",
       "      <td>guitar</td>\n",
       "      <td>look</td>\n",
       "      <td>water</td>\n",
       "      <td>talk</td>\n",
       "      <td>tell</td>\n",
       "      <td>eve</td>\n",
       "      <td>bee</td>\n",
       "      <td>watch</td>\n",
       "      <td>hive</td>\n",
       "      <td>stop</td>\n",
       "      <td>cave</td>\n",
       "      <td>fresh</td>\n",
       "      <td>dig</td>\n",
       "      <td>wreckage</td>\n",
       "      <td>surely</td>\n",
       "      <td>suicide</td>\n",
       "      <td>loo</td>\n",
       "      <td>beehive</td>\n",
       "      <td>irrational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>charlie</td>\n",
       "      <td>jack</td>\n",
       "      <td>antenna</td>\n",
       "      <td>dig</td>\n",
       "      <td>liam</td>\n",
       "      <td>moth</td>\n",
       "      <td>shaft</td>\n",
       "      <td>drive</td>\n",
       "      <td>kate</td>\n",
       "      <td>band</td>\n",
       "      <td>bloody</td>\n",
       "      <td>look</td>\n",
       "      <td>help</td>\n",
       "      <td>music</td>\n",
       "      <td>rock</td>\n",
       "      <td>confession</td>\n",
       "      <td>cocoon</td>\n",
       "      <td>position</td>\n",
       "      <td>wait</td>\n",
       "      <td>choice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inhaler</td>\n",
       "      <td>sawyer</td>\n",
       "      <td>asthma</td>\n",
       "      <td>medicine</td>\n",
       "      <td>peanut</td>\n",
       "      <td>butter</td>\n",
       "      <td>kiss</td>\n",
       "      <td>breathe</td>\n",
       "      <td>david</td>\n",
       "      <td>deal</td>\n",
       "      <td>money</td>\n",
       "      <td>shannon</td>\n",
       "      <td>tell</td>\n",
       "      <td>nose</td>\n",
       "      <td>triple</td>\n",
       "      <td>look</td>\n",
       "      <td>cave</td>\n",
       "      <td>000</td>\n",
       "      <td>attack</td>\n",
       "      <td>shore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alex</td>\n",
       "      <td>nadia</td>\n",
       "      <td>danielle</td>\n",
       "      <td>golf</td>\n",
       "      <td>bombing</td>\n",
       "      <td>sayid</td>\n",
       "      <td>safe</td>\n",
       "      <td>jazeem</td>\n",
       "      <td>robert</td>\n",
       "      <td>est치</td>\n",
       "      <td>d칩nde</td>\n",
       "      <td>est</td>\n",
       "      <td>rash</td>\n",
       "      <td>people</td>\n",
       "      <td>noor</td>\n",
       "      <td>music</td>\n",
       "      <td>photograph</td>\n",
       "      <td>look</td>\n",
       "      <td>tell</td>\n",
       "      <td>hurley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>claire</td>\n",
       "      <td>baby</td>\n",
       "      <td>thomas</td>\n",
       "      <td>child</td>\n",
       "      <td>tell</td>\n",
       "      <td>lance</td>\n",
       "      <td>raise</td>\n",
       "      <td>psychic</td>\n",
       "      <td>happen</td>\n",
       "      <td>mean</td>\n",
       "      <td>manifest</td>\n",
       "      <td>look</td>\n",
       "      <td>danger</td>\n",
       "      <td>pink</td>\n",
       "      <td>jack</td>\n",
       "      <td>plan</td>\n",
       "      <td>pregnant</td>\n",
       "      <td>littleton</td>\n",
       "      <td>sleepwalk</td>\n",
       "      <td>rom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      top1         top2       top3      top4       top5      top6     top7  \\\n",
       "0     help  transceiver        pen     shaft      drive   charlie  cockpit   \n",
       "1     bear      battery  iteration       guy      radio       sir      gun   \n",
       "2     wash        annie        gun      tell      cryin  dinosaur     lord   \n",
       "3    helen         boar      locke      tell  walkabout      fish     john   \n",
       "4    water         jack       rent      swim     father    claire      car   \n",
       "5   people       guitar       look     water       talk      tell      eve   \n",
       "6  charlie         jack    antenna       dig       liam      moth    shaft   \n",
       "7  inhaler       sawyer     asthma  medicine     peanut    butter     kiss   \n",
       "8     alex        nadia   danielle      golf    bombing     sayid     safe   \n",
       "9   claire         baby     thomas     child       tell     lance    raise   \n",
       "\n",
       "      top8      top9     top10      top11     top12    top13   top14  \\\n",
       "0     jack    rescue      beth       look      body      rip   nerve   \n",
       "1   french  seatbelt     polar     fasten   checker   signal    body   \n",
       "2     look      kate       dog  dangerous  mortgage     walk  signal   \n",
       "3     talk      rise    norman      randy   destiny  colonel    look   \n",
       "4     look      jump  latitude       talk    decide    crazy    fail   \n",
       "5      bee     watch      hive       stop      cave    fresh     dig   \n",
       "6    drive      kate      band     bloody      look     help   music   \n",
       "7  breathe     david      deal      money   shannon     tell    nose   \n",
       "8   jazeem    robert      est치      d칩nde       est     rash  people   \n",
       "9  psychic    happen      mean   manifest      look   danger    pink   \n",
       "\n",
       "      top15       top16       top17      top18      top19       top20  \n",
       "0     drape       shout         sew     monkey   seatbelt    bathroom  \n",
       "1       bar        tell        walt      waste       shut      excuse  \n",
       "2     fever        damn       water        die   organize     picture  \n",
       "3      food    croucher       climb      sorry       hunt    fuselage  \n",
       "4     chase     chrissy  wonderland  silverman       ugly   hairbrush  \n",
       "5  wreckage      surely     suicide        loo    beehive  irrational  \n",
       "6      rock  confession      cocoon   position       wait      choice  \n",
       "7    triple        look        cave        000     attack       shore  \n",
       "8      noor       music  photograph       look       tell      hurley  \n",
       "9      jack        plan    pregnant  littleton  sleepwalk         rom  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest = 20\n",
    "order = np.argsort(-tfidf_df.values, axis=1)[:, :nlargest]\n",
    "result = pd.DataFrame(tfidf_df.columns[order], \n",
    "                      columns=['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                      index=tfidf_df.index)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "nbep = len(result)\n",
    "for i in range(nbep):\n",
    "    lig_df = tfidf_df[i:i+1]\n",
    "    lig_res = result[i:i+1]\n",
    "\n",
    "    mots = list(np.array(lig_res)[0])\n",
    "    values = [float(lig_df[mot]) for mot in mots]\n",
    "    df = pd.DataFrame(np.column_stack([mots, values]), columns=['Word', 'Tfidf'])\n",
    "    df.Tfidf = pd.to_numeric(df.Tfidf)\n",
    "    df.sort_values(by ='Tfidf', inplace = True, ascending=True)\n",
    "    titre = \"top \"+str(nlargest)+\" tf-idf scores for Lost season 1 episode \"+str(i+1)\n",
    "    get_hist(df, \"Word\", \"Tfidf\", titre, \"orange\", horizontal=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
