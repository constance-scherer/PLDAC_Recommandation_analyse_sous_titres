{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='darkred'> Clustering sur les épisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import math\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def verifier_ligne(ligne):\n",
    "    \"\"\"return True si la ligne est un sous-titre, False sinon\"\"\"\n",
    "    timestamp_regex = r'[0-9]{2}:[0-9]{2}:[0-9]{2}' \n",
    "    subnumber_regex =r'^[0-9]+$'\n",
    "    \n",
    "    liste_regex = [timestamp_regex, subnumber_regex]\n",
    "\n",
    "    l = ligne.lower()\n",
    "    if \"addic7ed\" in l:\n",
    "        return False\n",
    "    #if l.startswith(\"sync\"):\n",
    "        #return False\n",
    "    for regex in liste_regex:\n",
    "        if re.match(regex, ligne):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def transformer_ligne(ligne):\n",
    "    \"\"\"str -> str\n",
    "    effectue transformation souhaitees sur la ligne\"\"\"\n",
    "    tag_regex = r'<(/)*[a-zA-Z]+>' #to get rif of tags\n",
    "    alphanum_regex = r'\\W+'  #get rid of non alphanumeric characters\n",
    "    new_line = re.sub(tag_regex, '', ligne)\n",
    "    new_line = re.sub(r'\\W+', ' ', new_line)\n",
    "    return new_line\n",
    "\n",
    "def scan_folder(parent_folder, corp):\n",
    "    \"\"\"retourne corpus des textes contenus dans parent_folder sous forme de liste de string\"\"\"\n",
    "    # iterate over all the files in directory 'parent_folder'\n",
    "\n",
    "    for file_name in sorted(os.listdir(parent_folder)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            path = parent_folder+\"/\"+file_name\n",
    "            fichier = open(path, \"r\", encoding = \"ISO-8859-1\")\n",
    "            lignes = fichier.readlines()\n",
    "            fichier.close()\n",
    "            \n",
    "            texte = \"\"\n",
    "            for ligne in lignes :\n",
    "                #ligne = ligne.lower()\n",
    "                if verifier_ligne(ligne):\n",
    "                    new_line = transformer_ligne(ligne)\n",
    "                    texte += new_line\n",
    "            corp.append(texte)\n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((parent_folder, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recall this method\n",
    "                scan_folder(current_path, corp)\n",
    "    \n",
    "    return corp\n",
    "\n",
    "\n",
    "#pour eviter les variables globales utiliser get_corpus qui appelle scan_folder\n",
    "def get_corpus(parent_folder):\n",
    "    \"\"\"retourne corpus des textes contenus dans parent_folder sous forme de liste de string\"\"\"\n",
    "    c = []\n",
    "    res = scan_folder(parent_folder, c)\n",
    "    return res\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    blob = TextBlob(str_input.lower())\n",
    "    tokens = blob.words\n",
    "    words = [token.stem() for token in tokens]\n",
    "    return words\n",
    "\n",
    "def lemmatizing_tokenizer(str_input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(str_input)\n",
    "    words = [lemmatizer.lemmatize(word, pos=\"v\") for word in tokens]\n",
    "    return words\n",
    "\n",
    "def lemmatizing_tokenizer_v2(str_input):\n",
    "    words = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens_tagged =pos_tag(word_tokenize(str_input))\n",
    "    for word, tag in tokens_tagged:\n",
    "        if tag.startswith(\"NN\"):\n",
    "            word = wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            word = wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            word = wnl.lemmatize(word, pos='a')\n",
    "        else:\n",
    "            pass\n",
    "        words.append(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = get_corpus(\"/Vrac/PLDAC_addic7ed/data/1___Lost/05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'> Ensemble de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "436"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/users/Etu6/3520166/PLDAC_Recommandation_analyse_sous_titres-master/ressources/stopwords.txt\"\n",
    "fichier = open(path, \"r\")\n",
    "lignes = fichier.readlines()\n",
    "fichier.close()\n",
    "stopwords_set = set()\n",
    "for l in lignes :\n",
    "    l = l.rstrip('\\n')\n",
    "    stopwords_set.add(l)\n",
    "len(stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Clustering par saison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut voir si l'algo des kMeans est capable de différencier les épisodes de la saison 1 d'Avatar de la saison 2 de la même série en se basant sur les tf-idf des mots par rapport aux deux saisons.\n",
    "\n",
    "On réalise deux corpus composés des deux saisons afin de calculer séparement les tf-idf des mots par saison (sinon, c'est beaucoup plus facile de différencier des épisodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation du corpus \n",
    "path_serie1 = \"/Vrac/PLDAC_addic7ed/data/175___Avatar__The_Last_Airbender/01\"\n",
    "path_serie2 = \"/Vrac/PLDAC_addic7ed/data/175___Avatar__The_Last_Airbender/02\"\n",
    "\n",
    "corpus1 = get_corpus(path_serie1)\n",
    "corpus2 = get_corpus(path_serie2)\n",
    "\n",
    "corpus = corpus1 + corpus2\n",
    "\n",
    "# ensemble de stopwords = stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X1 = vectorizer.fit_transform(corpus1)\n",
    "voc1 = vectorizer.get_feature_names()\n",
    "dense1 = X1.todense()\n",
    "denselist1 = dense1.tolist()\n",
    "df_tfidf1 = pd.DataFrame(denselist1, columns=voc1)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X2 = vectorizer.fit_transform(corpus2)\n",
    "voc2 = vectorizer.get_feature_names()\n",
    "dense2 = X2.todense()\n",
    "denselist2 = dense2.tolist()\n",
    "df_tfidf2 = pd.DataFrame(denselist2, columns=voc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_tfidf1, df_tfidf2]\n",
    "df_tfidf = pd.concat(frames, sort=True)\n",
    "df_tfidf = df_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels corrects pour calculer l'accuracy\n",
    "# 1 = saison 1\n",
    "# 0 = saison 2\n",
    "\n",
    "y = []\n",
    "for i in range(0, 21) :\n",
    "    y.append(1)\n",
    "for i in range(0, 20) :\n",
    "    y.append(0)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_tfidf = df_tfidf.values\n",
    "# pour la transformer en sparse matrix\n",
    "sparse_matrix_tfidf = sparse.csr_matrix(matrix_tfidf)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(sparse_matrix_tfidf)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 0 classé dans le groupe : 1\n",
      "Épisode 1 classé dans le groupe : 1\n",
      "Épisode 2 classé dans le groupe : 1\n",
      "Épisode 3 classé dans le groupe : 1\n",
      "Épisode 4 classé dans le groupe : 1\n",
      "Épisode 5 classé dans le groupe : 1\n",
      "Épisode 6 classé dans le groupe : 1\n",
      "Épisode 7 classé dans le groupe : 1\n",
      "Épisode 8 classé dans le groupe : 1\n",
      "Épisode 9 classé dans le groupe : 1\n",
      "Épisode 10 classé dans le groupe : 1\n",
      "Épisode 11 classé dans le groupe : 1\n",
      "Épisode 12 classé dans le groupe : 1\n",
      "Épisode 13 classé dans le groupe : 1\n",
      "Épisode 14 classé dans le groupe : 1\n",
      "Épisode 15 classé dans le groupe : 0\n",
      "Épisode 16 classé dans le groupe : 1\n",
      "Épisode 17 classé dans le groupe : 1\n",
      "Épisode 18 classé dans le groupe : 1\n",
      "Épisode 19 classé dans le groupe : 1\n",
      "Épisode 20 classé dans le groupe : 1\n",
      "Épisode 21 classé dans le groupe : 0\n",
      "Épisode 22 classé dans le groupe : 0\n",
      "Épisode 23 classé dans le groupe : 1\n",
      "Épisode 24 classé dans le groupe : 0\n",
      "Épisode 25 classé dans le groupe : 0\n",
      "Épisode 26 classé dans le groupe : 0\n",
      "Épisode 27 classé dans le groupe : 1\n",
      "Épisode 28 classé dans le groupe : 1\n",
      "Épisode 29 classé dans le groupe : 0\n",
      "Épisode 30 classé dans le groupe : 0\n",
      "Épisode 31 classé dans le groupe : 1\n",
      "Épisode 32 classé dans le groupe : 0\n",
      "Épisode 33 classé dans le groupe : 0\n",
      "Épisode 34 classé dans le groupe : 0\n",
      "Épisode 35 classé dans le groupe : 1\n",
      "Épisode 36 classé dans le groupe : 0\n",
      "Épisode 37 classé dans le groupe : 0\n",
      "Épisode 38 classé dans le groupe : 1\n",
      "Épisode 39 classé dans le groupe : 0\n",
      "Épisode 40 classé dans le groupe : 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(labels)) :\n",
    "    print(\"Épisode \"+str(i)+\" classé dans le groupe : \"+str(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8048780487804879\n"
     ]
    }
   ],
   "source": [
    "cpt = 0\n",
    "for i in range(0, len(labels)) :\n",
    "    if y[i] == labels[i] :\n",
    "        cpt += 1\n",
    "print(\"Accuracy = \"+str(cpt/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cet exemple, l'accuracy est bonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Clustering par série"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut voir la même chose qu'avant mais cette fois-ci entre les épisodes de deux séries différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'> Séries très différentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premiers tests sur deux séries très différentes :\n",
    "- Lost\n",
    "- Avatar\n",
    "\n",
    "On s'attend à de bons résultats : le vocabulaire employé dans les deux séries est très différent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation du corpus \n",
    "path_serie1 = \"/Vrac/PLDAC_addic7ed/data/175___Avatar__The_Last_Airbender/01\"\n",
    "path_serie2 = \"/Vrac/PLDAC_addic7ed/data/1___Lost/01\"\n",
    "\n",
    "corpus1 = get_corpus(path_serie1)\n",
    "corpus2 = get_corpus(path_serie2)\n",
    "\n",
    "corpus = corpus1 + corpus2\n",
    "\n",
    "# ensemble de stopwords = stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X1 = vectorizer.fit_transform(corpus1)\n",
    "voc1 = vectorizer.get_feature_names()\n",
    "dense1 = X1.todense()\n",
    "denselist1 = dense1.tolist()\n",
    "df_tfidf1 = pd.DataFrame(denselist1, columns=voc1)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X2 = vectorizer.fit_transform(corpus2)\n",
    "voc2 = vectorizer.get_feature_names()\n",
    "dense2 = X2.todense()\n",
    "denselist2 = dense2.tolist()\n",
    "df_tfidf2 = pd.DataFrame(denselist2, columns=voc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_tfidf1, df_tfidf2]\n",
    "df_tfidf = pd.concat(frames, sort=True)\n",
    "df_tfidf = df_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_tfidf = df_tfidf.values\n",
    "# pour la transformer en sparse matrix\n",
    "sparse_matrix_tfidf = sparse.csr_matrix(matrix_tfidf)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(sparse_matrix_tfidf)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 0 classé dans le groupe : 0\n",
      "Épisode 1 classé dans le groupe : 0\n",
      "Épisode 2 classé dans le groupe : 0\n",
      "Épisode 3 classé dans le groupe : 0\n",
      "Épisode 4 classé dans le groupe : 0\n",
      "Épisode 5 classé dans le groupe : 0\n",
      "Épisode 6 classé dans le groupe : 0\n",
      "Épisode 7 classé dans le groupe : 0\n",
      "Épisode 8 classé dans le groupe : 0\n",
      "Épisode 9 classé dans le groupe : 0\n",
      "Épisode 10 classé dans le groupe : 0\n",
      "Épisode 11 classé dans le groupe : 0\n",
      "Épisode 12 classé dans le groupe : 0\n",
      "Épisode 13 classé dans le groupe : 0\n",
      "Épisode 14 classé dans le groupe : 0\n",
      "Épisode 15 classé dans le groupe : 0\n",
      "Épisode 16 classé dans le groupe : 0\n",
      "Épisode 17 classé dans le groupe : 0\n",
      "Épisode 18 classé dans le groupe : 0\n",
      "Épisode 19 classé dans le groupe : 0\n",
      "Épisode 20 classé dans le groupe : 0\n",
      "Épisode 21 classé dans le groupe : 1\n",
      "Épisode 22 classé dans le groupe : 1\n",
      "Épisode 23 classé dans le groupe : 1\n",
      "Épisode 24 classé dans le groupe : 1\n",
      "Épisode 25 classé dans le groupe : 1\n",
      "Épisode 26 classé dans le groupe : 1\n",
      "Épisode 27 classé dans le groupe : 1\n",
      "Épisode 28 classé dans le groupe : 1\n",
      "Épisode 29 classé dans le groupe : 1\n",
      "Épisode 30 classé dans le groupe : 1\n",
      "Épisode 31 classé dans le groupe : 1\n",
      "Épisode 32 classé dans le groupe : 1\n",
      "Épisode 33 classé dans le groupe : 1\n",
      "Épisode 34 classé dans le groupe : 1\n",
      "Épisode 35 classé dans le groupe : 1\n",
      "Épisode 36 classé dans le groupe : 1\n",
      "Épisode 37 classé dans le groupe : 1\n",
      "Épisode 38 classé dans le groupe : 1\n",
      "Épisode 39 classé dans le groupe : 1\n",
      "Épisode 40 classé dans le groupe : 1\n",
      "Épisode 41 classé dans le groupe : 1\n",
      "Épisode 42 classé dans le groupe : 1\n",
      "Épisode 43 classé dans le groupe : 1\n",
      "Épisode 44 classé dans le groupe : 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(labels)) :\n",
    "    print(\"Épisode \"+str(i)+\" classé dans le groupe : \"+str(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "for i in range(0, len(corpus1)) :\n",
    "    y.append(0)\n",
    "for i in range(0, len(corpus2)) :\n",
    "    y.append(1)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "cpt = 0\n",
    "for i in range(0, len(labels)) :\n",
    "    if y[i] == labels[i] :\n",
    "        cpt += 1\n",
    "print(\"Accuracy = \"+str(cpt/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ces deux séries, classification parfaite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'> Séries très similaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse maintenant à deux séries avec un vocabulaire plus similaire.\n",
    "- Dr House\n",
    "- Grey's Anatomy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation du corpus \n",
    "path_serie1 =\"/Vrac/PLDAC_addic7ed/data/30___Grey_s_Anatomy/01\"\n",
    "path_serie2 = \"/Vrac/PLDAC_addic7ed/addic7ed/15___House/01\"\n",
    "\n",
    "corpus1 = get_corpus(path_serie1)\n",
    "corpus2 = get_corpus(path_serie2)\n",
    "\n",
    "corpus = corpus1 + corpus2\n",
    "\n",
    "# ensemble de stopwords = stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X1 = vectorizer.fit_transform(corpus1)\n",
    "voc1 = vectorizer.get_feature_names()\n",
    "dense1 = X1.todense()\n",
    "denselist1 = dense1.tolist()\n",
    "df_tfidf1 = pd.DataFrame(denselist1, columns=voc1)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X2 = vectorizer.fit_transform(corpus2)\n",
    "voc2 = vectorizer.get_feature_names()\n",
    "dense2 = X2.todense()\n",
    "denselist2 = dense2.tolist()\n",
    "df_tfidf2 = pd.DataFrame(denselist2, columns=voc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_tfidf1, df_tfidf2]\n",
    "df_tfidf = pd.concat(frames, sort=True)\n",
    "df_tfidf = df_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_tfidf = df_tfidf.values\n",
    "# pour la transformer en sparse matrix\n",
    "sparse_matrix_tfidf = sparse.csr_matrix(matrix_tfidf)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(sparse_matrix_tfidf)\n",
    "labels = kmeans.labels_\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 0 classé dans le groupe : 0\n",
      "Épisode 1 classé dans le groupe : 0\n",
      "Épisode 2 classé dans le groupe : 0\n",
      "Épisode 3 classé dans le groupe : 0\n",
      "Épisode 4 classé dans le groupe : 0\n",
      "Épisode 5 classé dans le groupe : 1\n",
      "Épisode 6 classé dans le groupe : 0\n",
      "Épisode 7 classé dans le groupe : 0\n",
      "Épisode 8 classé dans le groupe : 1\n",
      "Épisode 9 classé dans le groupe : 1\n",
      "Épisode 10 classé dans le groupe : 1\n",
      "Épisode 11 classé dans le groupe : 1\n",
      "Épisode 12 classé dans le groupe : 1\n",
      "Épisode 13 classé dans le groupe : 1\n",
      "Épisode 14 classé dans le groupe : 1\n",
      "Épisode 15 classé dans le groupe : 1\n",
      "Épisode 16 classé dans le groupe : 1\n",
      "Épisode 17 classé dans le groupe : 1\n",
      "Épisode 18 classé dans le groupe : 1\n",
      "Épisode 19 classé dans le groupe : 1\n",
      "Épisode 20 classé dans le groupe : 1\n",
      "Épisode 21 classé dans le groupe : 1\n",
      "Épisode 22 classé dans le groupe : 1\n",
      "Épisode 23 classé dans le groupe : 1\n",
      "Épisode 24 classé dans le groupe : 1\n",
      "Épisode 25 classé dans le groupe : 1\n",
      "Épisode 26 classé dans le groupe : 1\n",
      "Épisode 27 classé dans le groupe : 1\n",
      "Épisode 28 classé dans le groupe : 1\n",
      "Épisode 29 classé dans le groupe : 1\n",
      "Épisode 30 classé dans le groupe : 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(labels)) :\n",
    "    print(\"Épisode \"+str(i)+\" classé dans le groupe : \"+str(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "for i in range(0, len(corpus1)) :\n",
    "    y.append(0)\n",
    "for i in range(0, len(corpus2)) :\n",
    "    y.append(1)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9354838709677419\n"
     ]
    }
   ],
   "source": [
    "cpt = 0\n",
    "for i in range(0, len(labels)) :\n",
    "    if y[i] == labels[i] :\n",
    "        cpt += 1\n",
    "print(\"Accuracy = \"+str(cpt/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, les deux séries utilisent un vocabulaire similaire (vocabulaire médical en plus) ce qui peut expliquer la petite baisse en accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Problèmes des noms propres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de pimenter un peu la classification, on va essayer d'enlever les noms propres, qui sont très discriminants pour différencier deux épisodes de séries différentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formation du corpus \n",
    "path_serie1 =\"/Vrac/PLDAC_addic7ed/data/30___Grey_s_Anatomy/01\"\n",
    "path_serie2 = \"/Vrac/PLDAC_addic7ed/addic7ed/15___House/01\"\n",
    "\n",
    "corpus1 = get_corpus(path_serie1)\n",
    "corpus2 = get_corpus(path_serie2)\n",
    "\n",
    "corpus = corpus1 + corpus2\n",
    "\n",
    "# ensemble de stopwords = stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X1 = vectorizer.fit_transform(corpus1)\n",
    "voc1 = vectorizer.get_feature_names()\n",
    "dense1 = X1.todense()\n",
    "denselist1 = dense1.tolist()\n",
    "df_tfidf1 = pd.DataFrame(denselist1, columns=voc1)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X2 = vectorizer.fit_transform(corpus2)\n",
    "voc2 = vectorizer.get_feature_names()\n",
    "dense2 = X2.todense()\n",
    "denselist2 = dense2.tolist()\n",
    "df_tfidf2 = pd.DataFrame(denselist2, columns=voc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_tfidf1, df_tfidf2]\n",
    "df_tfidf = pd.concat(frames, sort=True)\n",
    "df_tfidf = df_tfidf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "      <th>top6</th>\n",
       "      <th>top7</th>\n",
       "      <th>top8</th>\n",
       "      <th>top9</th>\n",
       "      <th>top10</th>\n",
       "      <th>top11</th>\n",
       "      <th>top12</th>\n",
       "      <th>top13</th>\n",
       "      <th>top14</th>\n",
       "      <th>top15</th>\n",
       "      <th>top16</th>\n",
       "      <th>top17</th>\n",
       "      <th>top18</th>\n",
       "      <th>top19</th>\n",
       "      <th>top20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>james</td>\n",
       "      <td>twitch</td>\n",
       "      <td>tuberculoma</td>\n",
       "      <td>whoah</td>\n",
       "      <td>wrist</td>\n",
       "      <td>victoria</td>\n",
       "      <td>pin</td>\n",
       "      <td>mri</td>\n",
       "      <td>homeless</td>\n",
       "      <td>fury</td>\n",
       "      <td>meningitis</td>\n",
       "      <td>foreman</td>\n",
       "      <td>prozac</td>\n",
       "      <td>terharg</td>\n",
       "      <td>streets</td>\n",
       "      <td>cancer</td>\n",
       "      <td>treatment</td>\n",
       "      <td>doesn</td>\n",
       "      <td>sorry</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mark</td>\n",
       "      <td>alzheimer</td>\n",
       "      <td>attack</td>\n",
       "      <td>stacy</td>\n",
       "      <td>yoga</td>\n",
       "      <td>paris</td>\n",
       "      <td>blah</td>\n",
       "      <td>aip</td>\n",
       "      <td>mountain</td>\n",
       "      <td>encephalitis</td>\n",
       "      <td>gregg</td>\n",
       "      <td>warner</td>\n",
       "      <td>gonna</td>\n",
       "      <td>doesn</td>\n",
       "      <td>house</td>\n",
       "      <td>time</td>\n",
       "      <td>bladder</td>\n",
       "      <td>paranoia</td>\n",
       "      <td>friends</td>\n",
       "      <td>guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>meningitis</td>\n",
       "      <td>rash</td>\n",
       "      <td>form</td>\n",
       "      <td>mary</td>\n",
       "      <td>neck</td>\n",
       "      <td>blue</td>\n",
       "      <td>bleeding</td>\n",
       "      <td>ow</td>\n",
       "      <td>blood</td>\n",
       "      <td>interview</td>\n",
       "      <td>interviewing</td>\n",
       "      <td>brenda</td>\n",
       "      <td>lumbar</td>\n",
       "      <td>puncture</td>\n",
       "      <td>stomach</td>\n",
       "      <td>fine</td>\n",
       "      <td>people</td>\n",
       "      <td>asian</td>\n",
       "      <td>pool</td>\n",
       "      <td>shoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mom</td>\n",
       "      <td>dvt</td>\n",
       "      <td>schizophrenia</td>\n",
       "      <td>vitamin</td>\n",
       "      <td>luke</td>\n",
       "      <td>gonna</td>\n",
       "      <td>birthday</td>\n",
       "      <td>meds</td>\n",
       "      <td>dr</td>\n",
       "      <td>blood</td>\n",
       "      <td>ampicillin</td>\n",
       "      <td>schizophrenic</td>\n",
       "      <td>pt</td>\n",
       "      <td>pickles</td>\n",
       "      <td>tumor</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>crazy</td>\n",
       "      <td>services</td>\n",
       "      <td>praise</td>\n",
       "      <td>shrink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>matt</td>\n",
       "      <td>chi</td>\n",
       "      <td>hydrolase</td>\n",
       "      <td>davis</td>\n",
       "      <td>kid</td>\n",
       "      <td>poisoned</td>\n",
       "      <td>time</td>\n",
       "      <td>son</td>\n",
       "      <td>mom</td>\n",
       "      <td>tomato</td>\n",
       "      <td>cdc</td>\n",
       "      <td>clothes</td>\n",
       "      <td>poison</td>\n",
       "      <td>pesticides</td>\n",
       "      <td>washed</td>\n",
       "      <td>bus</td>\n",
       "      <td>gonna</td>\n",
       "      <td>detergent</td>\n",
       "      <td>pesticide</td>\n",
       "      <td>disulfoton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          top1       top2           top3     top4   top5      top6      top7  \\\n",
       "17       james     twitch    tuberculoma    whoah  wrist  victoria       pin   \n",
       "18        mark  alzheimer         attack    stacy   yoga     paris      blah   \n",
       "19  meningitis       rash           form     mary   neck      blue  bleeding   \n",
       "20         mom        dvt  schizophrenia  vitamin   luke     gonna  birthday   \n",
       "21        matt        chi      hydrolase    davis    kid  poisoned      time   \n",
       "\n",
       "    top8      top9         top10         top11          top12   top13  \\\n",
       "17   mri  homeless          fury    meningitis        foreman  prozac   \n",
       "18   aip  mountain  encephalitis         gregg         warner   gonna   \n",
       "19    ow     blood     interview  interviewing         brenda  lumbar   \n",
       "20  meds        dr         blood    ampicillin  schizophrenic      pt   \n",
       "21   son       mom        tomato           cdc        clothes  poison   \n",
       "\n",
       "         top14    top15    top16      top17      top18      top19       top20  \n",
       "17     terharg  streets   cancer  treatment      doesn      sorry         105  \n",
       "18       doesn    house     time    bladder   paranoia    friends         guy  \n",
       "19    puncture  stomach     fine     people      asian       pool       shoes  \n",
       "20     pickles    tumor  alcohol      crazy   services     praise      shrink  \n",
       "21  pesticides   washed      bus      gonna  detergent  pesticide  disulfoton  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlargest = 20\n",
    "order = np.argsort(-df_tfidf.values, axis=1)[:, :nlargest]\n",
    "result = pd.DataFrame(df_tfidf.columns[order], \n",
    "                      columns=['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                      index=df_tfidf.index)\n",
    "result.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les tf-idf les plus élevés sont très souvent des noms propres.\n",
    "\n",
    "Le but va être de les éliminer pour chaque épisode afin de voir si les kMeans peuvent différencier sur autre chose.\n",
    "\n",
    "Dans un premier temps, on enlève, pour chaque épisode, les 20 mots dont les tf-idf sont les plus grands. C'est peut-être un peu trop, on ajustera après.\n",
    "\n",
    "Il faut soit remplacer le tf-idf de ces mots par autre chose ou bien les enlever du vocabulaire pour tous les épisodes.\n",
    "- remplacer : par un 0 ? un nombre aléatoire ? autre chose ?\n",
    "        -> on veut que ces mots ne soient plus discriminants pour la classification\n",
    "- enlever : on récupère pour chaque épisode les 20 mots avec les plus grands tf-idf et on crée un ensemble puis on enlève tous ces mots dans tous les épisodes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'> Élimination du dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère pour chaque épisode les 20 mots avec les plus grands tf-idf et on crée un ensemble puis on enlève tous ces mots dans tous les épisodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noms = result.values\n",
    "set_noms_propres = set()\n",
    "for ep in noms :\n",
    "    for mot in ep :\n",
    "        set_noms_propres.add(mot)\n",
    "liste_noms_propres = list(set_noms_propres)\n",
    "len(liste_noms_propres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>007</th>\n",
       "      <th>00am</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100l65</th>\n",
       "      <th>100mg</th>\n",
       "      <th>101</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zebras</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zit</th>\n",
       "      <th>zits</th>\n",
       "      <th>zoo</th>\n",
       "      <th>ªclickâ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>0.15689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00       000      007      00am   03        10       100    100l65  \\\n",
       "0  0.000000  0.000000  0.00000  0.000000  0.0  0.012173  0.000000  0.021025   \n",
       "1  0.069274  0.000000  0.00000  0.017794  0.0  0.020605  0.000000  0.000000   \n",
       "2  0.000000  0.015689  0.15689  0.000000  0.0  0.010755  0.000000  0.000000   \n",
       "3  0.014648  0.000000  0.00000  0.000000  0.0  0.000000  0.076272  0.000000   \n",
       "4  0.063004  0.000000  0.00000  0.000000  0.0  0.000000  0.098415  0.000000   \n",
       "\n",
       "   100mg  101  ...  yummy  yup  zebra    zebras  zero  zeros  zit  zits  zoo  \\\n",
       "0    0.0  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0   \n",
       "1    0.0  0.0  ...    0.0  0.0    0.0  0.017794   0.0    0.0  0.0   0.0  0.0   \n",
       "2    0.0  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0   \n",
       "3    0.0  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0   \n",
       "4    0.0  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0   \n",
       "\n",
       "   ªclickâ  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 9251 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>03</th>\n",
       "      <th>10</th>\n",
       "      <th>100l65</th>\n",
       "      <th>100mg</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>...</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zebra</th>\n",
       "      <th>zebras</th>\n",
       "      <th>zero</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zit</th>\n",
       "      <th>zits</th>\n",
       "      <th>zoo</th>\n",
       "      <th>ªclickâ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012173</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 8780 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         00       000      00am   03        10    100l65  100mg  101  102  \\\n",
       "0  0.000000  0.000000  0.000000  0.0  0.012173  0.021025    0.0  0.0  0.0   \n",
       "1  0.069274  0.000000  0.017794  0.0  0.020605  0.000000    0.0  0.0  0.0   \n",
       "2  0.000000  0.015689  0.000000  0.0  0.010755  0.000000    0.0  0.0  0.0   \n",
       "3  0.014648  0.000000  0.000000  0.0  0.000000  0.000000    0.0  0.0  0.0   \n",
       "4  0.063004  0.000000  0.000000  0.0  0.000000  0.000000    0.0  0.0  0.0   \n",
       "\n",
       "   103  ...  yummy  yup  zebra    zebras  zero  zeros  zit  zits  zoo  ªclickâ  \n",
       "0  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0      0.0  \n",
       "1  0.0  ...    0.0  0.0    0.0  0.017794   0.0    0.0  0.0   0.0  0.0      0.0  \n",
       "2  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0      0.0  \n",
       "3  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0      0.0  \n",
       "4  0.0  ...    0.0  0.0    0.0  0.000000   0.0    0.0  0.0   0.0  0.0      0.0  \n",
       "\n",
       "[5 rows x 8780 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pour se débarasser des colonnes\n",
    "df_copy = df_tfidf.copy()\n",
    "\n",
    "for mot in liste_noms_propres :\n",
    "    df_copy = df_copy.drop(mot, axis=1)\n",
    "    \n",
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dico = vectorizer.get_feature_names()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df_tfidf = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_tfidf = df_copy.values\n",
    "# pour la transformer en sparse matrix\n",
    "sparse_matrix_tfidf = sparse.csr_matrix(matrix_tfidf)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(sparse_matrix_tfidf)\n",
    "labels = kmeans.labels_\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Épisode 0 classé dans le groupe : 0\n",
      "Épisode 1 classé dans le groupe : 0\n",
      "Épisode 2 classé dans le groupe : 0\n",
      "Épisode 3 classé dans le groupe : 0\n",
      "Épisode 4 classé dans le groupe : 0\n",
      "Épisode 5 classé dans le groupe : 0\n",
      "Épisode 6 classé dans le groupe : 0\n",
      "Épisode 7 classé dans le groupe : 0\n",
      "Épisode 8 classé dans le groupe : 0\n",
      "Épisode 9 classé dans le groupe : 0\n",
      "Épisode 10 classé dans le groupe : 0\n",
      "Épisode 11 classé dans le groupe : 0\n",
      "Épisode 12 classé dans le groupe : 0\n",
      "Épisode 13 classé dans le groupe : 0\n",
      "Épisode 14 classé dans le groupe : 0\n",
      "Épisode 15 classé dans le groupe : 0\n",
      "Épisode 16 classé dans le groupe : 0\n",
      "Épisode 17 classé dans le groupe : 0\n",
      "Épisode 18 classé dans le groupe : 0\n",
      "Épisode 19 classé dans le groupe : 0\n",
      "Épisode 20 classé dans le groupe : 0\n",
      "Épisode 21 classé dans le groupe : 0\n",
      "Épisode 22 classé dans le groupe : 0\n",
      "Épisode 23 classé dans le groupe : 0\n",
      "Épisode 24 classé dans le groupe : 0\n",
      "Épisode 25 classé dans le groupe : 0\n",
      "Épisode 26 classé dans le groupe : 0\n",
      "Épisode 27 classé dans le groupe : 0\n",
      "Épisode 28 classé dans le groupe : 1\n",
      "Épisode 29 classé dans le groupe : 0\n",
      "Épisode 30 classé dans le groupe : 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(labels)) :\n",
    "    print(\"Épisode \"+str(i)+\" classé dans le groupe : \"+str(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = []\n",
    "for i in range(0, len(corpus1)) :\n",
    "    y.append(1)\n",
    "for i in range(0, len(corpus2)) :\n",
    "    y.append(0)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6774193548387096\n"
     ]
    }
   ],
   "source": [
    "cpt = 0\n",
    "for i in range(0, len(labels)) :\n",
    "    if y[i] == labels[i] :\n",
    "        cpt += 1\n",
    "print(\"Accuracy = \"+str(cpt/len(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy assez mauvaise, dans les affectations, on peut voir que tous les épisodes sauf un sont classés dans le groupe 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='darkblue'> Des petites fonctions propres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, y) :\n",
    "    \"\"\"\n",
    "    labels : labels prédis \n",
    "    y : labels corrects\n",
    "    renvoie l'accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    cpt = 0\n",
    "    for i in range(0, len(labels)) :\n",
    "        if y[i] == labels[i] :\n",
    "            cpt += 1\n",
    "    \n",
    "    return cpt/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retirer_n_premiers(n, df_tfidf) :\n",
    "\n",
    "    nlargest = n\n",
    "    order = np.argsort(-df_tfidf.values, axis=1)[:, :nlargest]\n",
    "    result = pd.DataFrame(df_tfidf.columns[order], \n",
    "                          columns=['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                          index=df_tfidf.index)\n",
    "\n",
    "\n",
    "    noms = result.values\n",
    "    set_noms_propres = set()\n",
    "    for ep in noms :\n",
    "        for mot in ep :\n",
    "            set_noms_propres.add(mot)\n",
    "    liste_noms_propres = list(set_noms_propres)\n",
    "    len(liste_noms_propres)\n",
    "\n",
    "    # pour se débarasser des colonnes\n",
    "    df_copy = df_tfidf.copy()\n",
    "\n",
    "    for mot in liste_noms_propres :\n",
    "        df_copy = df_copy.drop(mot, axis=1)\n",
    "\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcul_labels_corrects(corpus1, corpus2, labels) :\n",
    "    \n",
    "    y = []\n",
    "    for i in range(0, len(corpus1)) :\n",
    "        y.append(1)\n",
    "    for i in range(0, len(corpus2)) :\n",
    "        y.append(0)\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classif_deux_series(path_serie1, path_serie2, stopwords_set, n=0) :\n",
    "    \"\"\"\n",
    "    pour classifier deux séries\n",
    "    on enlève les n plus grands tf-idf\n",
    "    \"\"\"\n",
    "    corpus1 = get_corpus(path_serie1)\n",
    "    corpus2 = get_corpus(path_serie2)\n",
    "    corpus = corpus1 + corpus2\n",
    "    \n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "    X1 = vectorizer.fit_transform(corpus1)\n",
    "    voc1 = vectorizer.get_feature_names()\n",
    "    dense1 = X1.todense()\n",
    "    denselist1 = dense1.tolist()\n",
    "    df_tfidf1 = pd.DataFrame(denselist1, columns=voc1)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "    X2 = vectorizer.fit_transform(corpus2)\n",
    "    voc2 = vectorizer.get_feature_names()\n",
    "    dense2 = X2.todense()\n",
    "    denselist2 = dense2.tolist()\n",
    "    df_tfidf2 = pd.DataFrame(denselist2, columns=voc2)\n",
    "    \n",
    "    frames = [df_tfidf1, df_tfidf2]\n",
    "    df_tfidf = pd.concat(frames, sort=True)\n",
    "    df_tfidf = df_tfidf.fillna(0)\n",
    "    \n",
    "    if(n != 0) :\n",
    "        df_tfidf = retirer_n_premiers(n, df_tfidf)\n",
    "    \n",
    "    matrix_tfidf = df_tfidf.values\n",
    "    # pour la transformer en sparse matrix\n",
    "    sparse_matrix_tfidf = sparse.csr_matrix(matrix_tfidf)\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0, n_jobs=-1).fit(sparse_matrix_tfidf)\n",
    "    \n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    y = calcul_labels_corrects(corpus1, corpus2, labels)\n",
    "    \n",
    "    return labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6829268292682927"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_serie1 = \"/Vrac/PLDAC_addic7ed/data/175___Avatar__The_Last_Airbender/01\"\n",
    "path_serie2 = \"/Vrac/PLDAC_addic7ed/data/175___Avatar__The_Last_Airbender/02\"\n",
    "\n",
    "l, y = classif_deux_series(path_serie1, path_serie2, stopwords_set)\n",
    "accuracy(l, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5b97470e80>]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 20\n",
    "x = range(0, n)\n",
    "acc = []\n",
    "for i in x :\n",
    "    l, y = classif_deux_series(path_serie1, path_serie2, stopwords_set, i)\n",
    "    acc.append(accuracy(l, y))\n",
    "\n",
    "plt.plot(x, acc, color='deeppink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
