{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaisons entre différentes manières de prédiction de notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook présente la MSE et la MAE pour différentes façons de prédire les notes d'utilisateurs sur des séries.  \n",
    "4000 utilisateurs pour 3000 séries.  \n",
    "Au moins 4 notes pour chacun des utilisateurs.  \n",
    "Les différents algorithmes comparés :\n",
    "- kSVD\n",
    "- NMF\n",
    "    - sans biais\n",
    "    - avec biais\n",
    "- content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.predictions_content import *\n",
    "from utils.predictions_notes import *\n",
    "from utils.collaborative import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ratings = \"/Vrac/PLDAC_addic7ed/ratings/ratings_imdb/users\"\n",
    "#path_ratings = \"/Users/constancescherer/Desktop/ratings/ratings_imdb/users\"\n",
    "path_d_user = \"/Vrac/PLDAC_addic7ed/pickles/d_user.p\"\n",
    "#path_d_user = \"/Users/constancescherer/Desktop/pickles/d_user.p\"\n",
    "#d_user= get_d_user(path)\n",
    "with open(path_d_user, 'rb') as pickle_file:\n",
    "    d_user = pickle.load(pickle_file)\n",
    "\n",
    "liste_series = get_liste_series(d_user)\n",
    "data = get_data(d_user)\n",
    "all_data, num_user, num_item = get_all_data(data)\n",
    "train, train_mat, test = get_train_test(num_user, num_item, all_data, test_size=20)\n",
    "mean, u_means, i_means,U_ksvd, I_ksvd =  get_Uksvd_Iksvd(train, train_mat, num_user, num_item)\n",
    "d_username_id, d_itemname_id, Full = create_sparse_mat(data)\n",
    "\n",
    "#path_series = \"/Users/constancescherer/Desktop/addic7ed_final\"\n",
    "path_series = '/Vrac/PLDAC_addic7ed/addic7ed_final'\n",
    "d_info, d_name = getDicts(path_series)\n",
    "d_ind = reverse_dict(d_name)\n",
    "d_titre_filename = get_d_titre_filename(\"titles/title-filename.txt\")\n",
    "d_filename_titre = reverse_dict(d_titre_filename)\n",
    "d_id_username = reverse_dict(d_username_id)\n",
    "d_id_serie = reverse_dict(d_itemname_id)\n",
    "\n",
    "path_sim = \"/Vrac/PLDAC_addic7ed/pickles/sim.p\"\n",
    "#path_sim = \"/Users/constancescherer/Desktop/pickles/sim.p\"\n",
    "# matrice des similarités cosinus\n",
    "with open(path_sim, 'rb') as pickle_file:\n",
    "    similarities = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_ksvd(train_mat, test) :\n",
    "    ## Getting the truth values\n",
    "    truth_tr = np.array([rating for (uid,iid),rating in train_mat.items()])\n",
    "    truth_te = np.array([rating for uid,iid,rating in test])\n",
    "\n",
    "    prediction_tr = np.array([pred_func_ksvd(u,i, U_ksvd, I_ksvd, u_means, i_means, mean) for (u,i),rating in train_mat.items()])\n",
    "    prediction_te = np.array([pred_func_ksvd(u,i, U_ksvd, I_ksvd, u_means, i_means, mean) for u,i,rating in test])\n",
    "\n",
    "\n",
    "    print(\"Training Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "    print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "\n",
    "    print(\"Test Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "    print(\"MAE:\",  MAE_err(prediction_te,truth_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_NMF(train_mat, test, num_user, num_item) :\n",
    "    ## Getting the truth values\n",
    "    truth_tr = np.array([rating for (uid,iid),rating in train_mat.items()])\n",
    "    truth_te = np.array([rating for uid,iid,rating in test])\n",
    "    \n",
    "    prediction_tr, prediction_te = predictions_NMF(train_mat,test, 100, num_user, num_item)\n",
    "    print(\"Training Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "    print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "\n",
    "    print(\"Test Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "    print(\"MAE:\",  MAE_err(prediction_te,truth_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_NMF_biais(train_mat,test, nb_comp, num_user, num_item) :\n",
    "    truth_tr = np.array([rating for (uid,iid),rating in train_mat.items()])\n",
    "    truth_te = np.array([rating for uid,iid,rating in test])\n",
    "    \n",
    "    prediction_tr, prediction_te = predictions_NMF_biais(train_mat,test, 100, num_user, num_item)\n",
    "    print(\"Training Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "    print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "\n",
    "    print(\"Test Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "    print(\"MAE:\",  MAE_err(prediction_te,truth_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_content(train_mat, test, d_name, d_user, d_ind, d_titre_filename, d_filename_titre, d_id_username, d_id_serie, similarities) :\n",
    "    ## Getting the truth values\n",
    "    truth_tr = np.array([rating for (uid,iid),rating in train_mat.items()])\n",
    "    truth_te = np.array([rating for uid,iid,rating in test])\n",
    "\n",
    "    prediction_tr = np.array([pred_content(u, i, d_name, d_user, d_ind, d_titre_filename, d_filename_titre, d_id_username, d_id_serie, similarities) for (u,i),rating in train_mat.items()])\n",
    "    prediction_te = np.array([pred_content(u, i, d_name, d_user, d_ind, d_titre_filename, d_filename_titre, d_id_username, d_id_serie, similarities) for u,i,rating in test])\n",
    "\n",
    "\n",
    "    print(\"Training Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_tr,truth_tr))\n",
    "    print(\"MAE:\",  MAE_err(prediction_tr,truth_tr))\n",
    "\n",
    "    print(\"Test Error:\")\n",
    "    print(\"MSE:\",  MSE_err(prediction_te,truth_te))\n",
    "    print(\"MAE:\",  MAE_err(prediction_te,truth_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:\n",
      "MSE: 1.4717542445892757\n",
      "MAE: 0.7410327440155239\n",
      "Test Error:\n",
      "MSE: 5.641115231844463\n",
      "MAE: 1.716289295576649\n"
     ]
    }
   ],
   "source": [
    "error_ksvd(train_mat, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sans biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:\n",
      "MSE: 0.0029776867\n",
      "MAE: 0.0029776867\n",
      "Test Error:\n",
      "MSE: 8.944893460690668\n",
      "MAE: 2.3056576047024246\n"
     ]
    }
   ],
   "source": [
    "error_NMF(train_mat, test, num_user, num_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avec biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/nfs/Etu6/3520166/PLDAC_Recommandation_analyse_sous_titres-master/utils/collaborative.py:180: RuntimeWarning: Mean of empty slice.\n",
      "  item_mean = np.array(list(item.values())).mean()\n",
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:\n",
      "MSE: 2.0061487296492517\n",
      "MAE: 0.6923314900034804\n",
      "Test Error:\n",
      "MSE: 6.0911094783247615\n",
      "MAE: 1.6869948567229978\n"
     ]
    }
   ],
   "source": [
    "error_NMF_biais(train_mat,test, 100, num_user, num_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:\n",
      "MSE: 3.667719384181738\n",
      "MAE: 1.3562782783556986\n",
      "Test Error:\n",
      "MSE: 3.714425667401421\n",
      "MAE: 1.3764388929708549\n"
     ]
    }
   ],
   "source": [
    "error_content(train_mat, test, d_name, d_user, d_ind, d_titre_filename, d_filename_titre, d_id_username, d_id_serie, similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kSVD et content donnent de bons résultats en test. La NMF sans biais est un peu moins bonne en test mais elle ne prend pas en compte les biais des utilisateurs et des items (contrairement au kSVD). Avec la prise en compte des bias, les résultats sont un peu améliorés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
