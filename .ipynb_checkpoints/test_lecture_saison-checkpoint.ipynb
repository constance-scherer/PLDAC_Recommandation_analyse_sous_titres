{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(parent_folder):\n",
    "    \"\"\"retourne corpus des textes contenus dans parent_folder sous forme de liste de string\"\"\"\n",
    "    corpus = []\n",
    "    # iterate over all the files in directory 'parent_folder'\n",
    "    for file_name in os.listdir(parent_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            path = parent_folder+\"/\"+file_name\n",
    "            fichier = open(path, \"r\")\n",
    "            lignes = fichier.readlines()\n",
    "            fichier.close()\n",
    "            \n",
    "            texte = \"\"\n",
    "            for ligne in lignes :\n",
    "                if ligne[0] not in \"0123456789\":\n",
    "                    texte += ligne\n",
    "            corpus.append(texte)\n",
    "        \n",
    "        else:\n",
    "            current_path = \"\".join((parent_folder, \"/\", file_name))\n",
    "            if os.path.isdir(current_path):\n",
    "                # if we're checking a sub-directory, recall this method\n",
    "                scan_folder(current_path)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def somme_lignes(tab):\n",
    "    \"\"\"somme les lignes d'un tableau\"\"\"\n",
    "    res = []\n",
    "    for i in range(len(tab[0])):\n",
    "        res.append(0)\n",
    "    for k in range(len(tab)):\n",
    "        for i in range(len(tab[0])):\n",
    "                res[i] += tab[k][i]\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_n_plus_presents(n, corpus, stopwords_set):\n",
    "    \"\"\"dataFrame des n mots les plus present dans le corpus\"\"\"\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords_set)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    dico = vectorizer.get_feature_names()\n",
    "    nb_occ = somme_lignes(X.toarray())\n",
    "    \n",
    "    ind = np.argpartition(nb_occ, -n)[-n:]\n",
    "    ind = ind[np.argsort(-nb_occ[ind])]\n",
    "    words = [dico[i] for i in ind]\n",
    "\n",
    "    words_count = []\n",
    "    i = 0\n",
    "    for i in range(len(words)):\n",
    "        words_count.append(nb_occ[ind[i]])\n",
    "\n",
    "    df = pd.DataFrame(np.column_stack([words, words_count]), columns=['Word', 'Nb_occ'])\n",
    "    df.Nb_occ=pd.to_numeric(df.Nb_occ)\n",
    "    \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist(df, x_axis, y_axis, titre, colour, font_size=None, horizontal=False):\n",
    "    if horizontal:\n",
    "        hist = df.plot.barh(x=x_axis, y=y_axis, color=colour, title =titre, fontsize = font_size, edgecolor = \"none\").get_figure()\n",
    "    else:\n",
    "        hist = df.plot.bar(x=x_axis, y=y_axis, color=colour, title =titre, fontsize = font_size, edgecolor = \"none\").get_figure()\n",
    "    path_fig = \"img/\"+titre+'.png'\n",
    "    hist.savefig(path_fig,  bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition de l'ensemble de stopwords\n",
    "nltk_sw = set(stopwords.words('english'))\n",
    "sklearn_sw = set(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopwords_set = nltk_sw | sklearn_sw\n",
    "l_nb = [str(i) for i in range(1000000)]\n",
    "l_mots = [\"don\", \"yeah\", \"hey\", \"okay\", \"oh\", \"uh\", \"yes\", \"ok\"]\n",
    "for mot in l_mots :\n",
    "    stopwords_set.add(mot)\n",
    "for nb in l_nb:\n",
    "    stopwords_set.add(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/1___Lost/01'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e44f7336c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/1___Lost/01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_n_plus_presents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtitre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"les \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" mots les plus presents dans la premiere saison de Lost\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_hist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Nb_occ\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"teal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9ab4d6b150d6>\u001b[0m in \u001b[0;36mget_corpus\u001b[0;34m(parent_folder)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# iterate over all the files in directory 'parent_folder'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_folder\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/1___Lost/01'"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "corpus = get_corpus(\"data/1___Lost/01\")\n",
    "df_count = df_n_plus_presents(n, corpus, stopwords_set)\n",
    "titre = \"les \"+str(n)+\" mots les plus presents dans la premiere saison de Lost\"\n",
    "get_hist(df_count, \"Word\", \"Nb_occ\", titre, \"teal\", 7)\n",
    "df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stopwords_set)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dico = vectorizer.get_feature_names()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df_tfidf = pd.DataFrame(denselist, columns=feature_names)\n",
    "df_tfidf.loc[:,'gonna':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlargest = 20\n",
    "order = np.argsort(-df_tfidf.values, axis=1)[:, :nlargest]\n",
    "result = pd.DataFrame(df_tfidf.columns[order], \n",
    "                      columns=['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                      index=df_tfidf.index)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbep = 1\n",
    "for i in range(nbep):\n",
    "    lig_df = df_tfidf[i:i+1]\n",
    "    lig_res = result[i:i+1]\n",
    "\n",
    "    mots = list(np.array(lig_res)[0])\n",
    "    values = [float(lig_df[mot]) for mot in mots]\n",
    "    df = pd.DataFrame(np.column_stack([mots, values]), columns=['Word', 'Tfidf'])\n",
    "    df.Tfidf = pd.to_numeric(df.Tfidf)\n",
    "    df.sort_values(by ='Tfidf', inplace = True, ascending=True)\n",
    "    titre = \"top \"+str(nlargest)+\" tf-idf scores for Lost season 1 episode \"+str(i+1)\n",
    "    get_hist(df, \"Word\", \"Tfidf\", titre, \"limegreen\", horizontal=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
