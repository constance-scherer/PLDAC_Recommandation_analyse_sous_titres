{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from utils.swSets import *\n",
    "from utils.preprocessing import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pickle.load(open(\"/Vrac/PLDAC_reco/pickles/corpus.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Vrac/PLDAC_reco/addic7ed\"\n",
    "d_info, d_name = getDicts(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(corpus, stop_words = stopwords_set, max_df=0.4, min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X, open(\"/Vrac/PLDAC_reco/pickles/mat_count_voc_53540_maxdf_0.4_mindf_20.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pickle.load(open(\"/Vrac/PLDAC_reco/pickles/mat_count_voc_53540_maxdf_0.4_mindf_20.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(voc, open(\"/Vrac/PLDAC_reco/pickles/voc_count_voc_53540_maxdf_0.4_mindf_20.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53540"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "lda = LatentDirichletAllocation(n_components=k,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda.fit(X)\n",
    "doc_topic_distrib = lda.transform(X) #p(z|d)\n",
    "topic_word_distrib = lda.components_/lda.components_.sum(axis=1)[:, np.newaxis] #p(w|z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0\n",
      "['colonel', 'lieutenant', 'commander', 'sergeant', 'max', 'tony', 'smith', 'gibbs', 'archie', 'navy', 'roger', 'homer', 'admiral', 'edith', 'jeannie', 'aye', 'bart', 'macgyver', 'corporal', 'mcgee', 'groans', 'marge', 'lisa', 'radar', 'hogan', 'chattering', 'herr', 'schultz', 'petty', 'nelson', 'gasps', 'rene', 'grunts', 'lt', 'grunting', 'chuckling', 'marine', 'sonny', 'simpson', 'mac', 'doc', 'klink', 'groaning', 'skipper', 'pierce', 'crockett', 'german', 'bud', 'beeping', 'jim']\n",
      "cluster 1\n",
      "['sam', 'fred', 'lucy', 'harry', 'annie', 'lt', 'magnum', 'higgins', 'ted', 'niles', 'barney', 'betty', 'thomas', 'robin', 'dorothy', 'frasier', 'crane', 'darrin', 'larry', 'blanche', 'ralph', 'arthur', 'ethel', 'daphne', 'sheriff', 'rick', 'sally', 'alice', 'howard', 'leonard', 'samantha', 'billy', 'sheldon', 'mason', 'terrific', 'marshall', 'fran', 'telephone', 'stanley', 'lf', 'gee', 'jonathan', 'doorbell', 'freddie', 'roz', 'simon', 'sophia', 'scooby', 'maurice', 'penny']\n",
      "cluster 2\n",
      "['alex', 'ricky', 'amy', 'ben', 'dylan', 'rick', 'vampire', 'scott', 'elena', 'vince', 'adrian', 'tara', 'harvey', 'harper', 'logan', 'damon', 'clay', 'vampires', 'julian', 'stefan', 'buffy', 'wolf', 'jax', 'ari', 'makeup', 'jessica', 'matt', 'bonnie', 'tyler', 'klaus', 'jeremy', 'lizzie', 'evan', 'glenn', 'grace', 'witch', 'nora', 'werewolf', 'gemma', 'josh', 'ryan', 'nikita', 'caroline', 'ethan', 'creature', 'previously', 'justin', 'katherine', 'cure', 'sarah']\n",
      "cluster 3\n",
      "['sarah', 'cia', 'senator', 'russian', 'intelligence', 'secretary', 'washington', 'chuck', 'terrorist', 'agents', 'mike', 'walker', 'campaign', 'nuclear', 'americans', 'spy', 'al', 'minister', 'agency', 'bauer', 'intel', 'sydney', 'federal', 'casey', 'fbi', 'political', 'terrorists', 'tony', 'soldiers', 'virus', 'vice', 'network', 'data', 'sam', 'nation', 'files', 'satellite', 'charlie', 'quinn', 'china', 'device', 'morgan', 'operations', 'scott', 'election', 'conference', 'defense', 'simon', 'facility', 'division']\n",
      "cluster 4\n",
      "['henry', 'roman', 'william', 'france', 'richard', 'majesty', 'rome', 'german', 'charles', 'britain', 'royal', 'empire', 'monsieur', 'edward', 'elizabeth', 'thomas', 'minister', 'castle', 'madam', 'poirot', 'holmes', 'madame', 'germany', 'christian', 'soldiers', 'paris', 'lovejoy', 'francis', 'hitler', 'grace', 'scotland', 'thou', 'palace', 'germans', 'pope', 'political', 'emperor', 'robert', 'watson', 'religious', 'mistress', 'anne', 'duke', 'thy', 'louis', 'greek', 'honour', 'nation', 'union', 'troops']\n",
      "cluster 5\n",
      "['lt', 'commander', 'lf', 'alien', 'data', 'galaxy', 'systems', 'planets', 'humans', 'ships', 'ls', 'ln', 'mars', 'solar', 'device', 'freeman', 'orbit', 'lieutenant', 'colonel', 'beam', 'radiation', 'warp', 'aboard', 'aye', 'pilot', 'aliens', 'enterprise', 'gravity', 'vessel', 'alright', 'federation', 'shuttle', 'atmosphere', 'launch', 'core', 'sensors', 'jo', 'tardis', 'starfleet', 'species', 'worlds', 'bay', 'spock', 'fleet', 'jin', 'alpha', 'quantum', 'robot', 'transport', 'quarters']\n",
      "cluster 6\n",
      "['hank', 'bobby', 'emma', 'archer', 'leo', 'phoebe', 'laura', 'lana', 'oliver', 'piper', 'ellen', 'ross', 'paige', 'pam', 'ewing', 'jason', 'pete', 'artie', 'duncan', 'bo', 'mac', 'johnny', 'cliff', 'sue', 'cyril', 'ava', 'carol', 'macleod', 'cole', 'rachel', 'dale', 'ellie', 'barnes', 'dallas', 'katie', 'richie', 'felix', 'charlie', 'daniel', 'becca', 'patty', 'peggy', 'texas', 'hood', 'ingrid', 'kane', 'dd', 'tru', 'britt', 'evan']\n",
      "cluster 7\n",
      "['detective', 'jane', 'jury', 'inspector', 'sergeant', 'lieutenant', 'ms', 'jake', 'attorney', 'amy', 'defendant', 'motive', 'danny', 'fletcher', 'witnesses', 'alibi', 'homicide', 'eddie', 'nick', 'monk', 'objection', 'warrant', 'testify', 'grace', 'tommy', 'robbery', 'defense', 'murderer', 'lou', 'clients', 'testimony', 'detectives', 'bail', 'commissioner', 'steve', 'prints', 'jessica', 'ben', 'murders', 'rape', 'jim', 'custody', 'squad', 'scott', 'fingerprints', 'harry', 'district', 'officers', 'files', 'leslie']\n",
      "cluster 8\n",
      "['grunts', 'clark', 'grunting', 'rex', 'lois', 'batman', 'mulder', 'turtles', 'gasps', 'ninja', 'ben', 'beeping', 'groans', 'april', 'chloe', 'alien', 'lex', 'mutant', 'growling', 'scully', 'f1nc0', 'beast', 'groaning', 'superman', 'hulk', 'growls', 'robot', 'correction', 'spider', 'shredder', 'heh', 'heroes', 'turtle', 'robin', 'knight', 'mayor', 'uhh', 'gwen', 'creature', 'kent', 'panting', 'kevin', 'tess', 'leonardo', 'michelangelo', 'donatello', 'tires', 'stark', 'jedi', 'roars']\n",
      "cluster 9\n",
      "['mike', 'al', 'andy', 'steve', 'jim', 'kevin', 'doug', 'donna', 'joey', 'jerry', 'gary', 'shawn', 'phil', 'jackie', 'kelly', 'charlie', 'tim', 'eric', 'matt', 'grace', 'carrie', 'danny', 'dan', 'grandma', 'ed', 'lt', 'holly', 'mommy', 'com', 'luke', 'robert', 'tommy', 'ryan', 'couch', 'coach', 'karen', 'taylor', 'sue', 'julie', 'claire', 'justin', 'gus', 'liz', 'bobby', 'sean', 'jimmy', 'brandon', 'neighborhood', 'dawson', 'alan']\n",
      "cluster 10\n",
      "['patients', 'cancer', 'trauma', 'booth', 'ms', 'pulse', 'doc', 'abby', 'surgeon', 'carter', 'liver', 'dna', 'eli', 'sam', 'procedure', 'tube', 'clinic', 'infection', 'lungs', 'denny', 'symptoms', 'brennan', 'jordan', 'therapy', 'monitor', 'tumor', 'tissue', 'owen', 'alex', 'injury', 'jared', 'beeping', 'injuries', 'bp', 'alicia', 'nurses', 'surgical', 'bailey', 'richard', 'transplant', 'medication', 'exam', 'callie', 'pills', 'elliot', 'kidney', 'lung', 'meredith', 'stable', 'grey']\n",
      "cluster 11\n",
      "['sam', 'detective', 'fbi', 'dna', 'victims', 'danny', 'dean', 'ryan', 'daniel', 'charlie', 'henry', 'nick', 'corrected', 'emily', 'amanda', 'walter', 'warrant', 'parker', 'nate', 'kyle', 'eric', 'sheriff', 'nathan', 'max', 'indistinct', 'prints', 'boyd', 'sara', 'files', 'vic', 'grunts', 'olivia', 'mike', 'beeps', 'jimmy', 'vehicle', 'kate', 'tires', 'cellphone', 'federal', 'surveillance', 'ms', 'gunshot', 'tyler', 'connor', 'miami', 'siren', 'abby', 'shooter', 'footage']\n",
      "cluster 12\n",
      "['narrator', 'scientists', 'species', 'forest', 'jamie', 'myth', 'experiment', 'temperature', 'plants', 'desert', 'billion', 'largest', 'vast', 'robert', 'humans', 'impact', 'discovery', 'steel', 'fuel', 'structure', 'africa', 'trench', 'creatures', 'mountains', 'waves', 'phil', 'landscape', 'cells', 'scientific', 'britain', 'metres', 'tory', 'objects', 'enormous', 'carbon', 'formed', 'valley', 'cave', 'population', 'evolution', 'atmosphere', 'remarkable', 'creating', 'liquid', 'roads', 'islands', 'ancestors', 'survival', 'centre', 'creature']\n",
      "cluster 13\n",
      "['mum', 'cos', 'alright', 'erm', 'shit', 'bloke', 'oi', 'reckon', 'mobile', 'alan', 'pub', 'martin', 'realise', 'arse', 'sean', 'tony', 'harry', 'simon', 'aye', 'stephen', 'favourite', 'ben', 'lads', 'nick', 'charlie', 'dave', 'hiya', 'sarah', 'quid', 'lad', 'favour', 'mates', 'colin', 'ian', 'rubbish', 'richard', 'rachel', 'andy', 'gary', 'centre', 'innit', 'jim', 'dan', 'mummy', 'danny', 'dunno', 'brian', 'roger', 'sorted', 'helen']\n",
      "cluster 14\n",
      "['bleep', 'chef', 'announcer', 'dish', 'lisa', 'trump', 'cooked', 'ramsay', 'chefs', 'gordon', 'sauce', 'jon', 'crab', 'dishes', 'product', 'pork', 'lamb', 'josh', 'raw', 'salad', 'menu', 'scallops', 'flavor', 'beef', 'ingredients', 'risotto', 'salmon', 'steak', 'pan', 'lobster', 'teams', 'judges', 'dessert', 'pasta', 'fried', 'rice', 'masterchef', 'scott', 'oven', 'flavors', 'customers', 'shrimp', 'cooks', 'potatoes', 'stephen', 'bret', 'italian', 'robert', 'elimination', 'garnish']\n",
      "cluster 15\n",
      "['jake', 'max', 'gasps', 'lily', 'stan', 'yay', 'charlie', 'corrected', 'ben', 'candy', 'bro', 'groans', 'sam', 'eddie', 'gross', 'ba', 'boo', 'grunts', 'santa', 'finn', 'blah', 'doo', 'alan', 'amy', 'pee', 'lemon', 'heh', 'ted', 'costume', 'ew', 'www', 'pinky', 'grandma', 'mommy', 'grandpa', 'jerk', 'corrections', 'jimmy', 'clears', 'halloween', 'creepy', 'lame', 'bean', 'ding', 'amber', 'penis', 'robot', 'synced', 'woo', 'wha']\n",
      "cluster 16\n",
      "['adam', 'chris', 'jeff', 'blake', 'tribe', 'ashley', 'ali', 'bah', 'rachel', 'steven', 'votes', 'immunity', 'julie', 'teams', 'rob', 'erica', 'veto', 'mike', 'donaldson', 'tribal', 'carrington', 'dan', 'voted', 'matt', 'emily', 'wah', 'brendon', 'council', 'bleep', 'gary', 'coach', 'targets', 'colby', 'alliance', 'survivor', 'brad', 'russell', 'alexis', 'dave', 'natalie', 'elimination', 'austin', 'kyle', 'kelly', 'britney', 'keith', 'idol', 'amanda', 'jenna', 'jay']\n",
      "cluster 17\n",
      "['shit', 'fucking', 'fuck', 'bullshit', 'dick', 'fuckin', 'goddamn', 'asshole', 'fucked', 'larry', 'alright', 'dan', 'tommy', 'pussy', 'motherfucker', 'kenny', 'shane', 'jimmy', 'brian', 'bro', 'tony', 'marty', 'johnny', 'jenny', 'cock', 'prick', 'steve', 'tits', 'terry', 'blair', 'weed', 'chuck', 'whore', 'richard', 'kevin', 'becky', 'louie', 'roseanne', 'kyle', 'billy', 'charlie', 'cancer', 'goin', 'penis', 'shitty', 'randy', 'carl', 'bitches', 'nigga', 'michelle']\n",
      "cluster 18\n",
      "['iike', 'demon', 'gods', 'aii', 'lt', 'ii', 'xena', 'dragon', 'sword', 'hercules', 'sheriff', 'witch', 'warrior', 'san', 'weii', 'demons', 'darkness', 'merlin', 'duel', 'arthur', 'monsters', 'gabrielle', 'curse', 'wiii', 'kingdom', 'kun', 'wouid', 'grunts', 'guards', 'twilight', 'kai', 'summon', 'destiny', 'defeat', 'grunting', 'temple', 'couid', 'castle', 'sire', 'spirits', 'iet', 'iook', 'richard', 'forest', 'sam', 'witches', 'beast', 'gasps', 'lf', 'souls']\n",
      "cluster 19\n",
      "['judges', 'nick', 'vegas', 'brooke', 'christina', 'coach', 'hollywood', 'nigel', 'runway', 'singer', 'lauren', 'songs', 'studio', 'simon', 'audition', 'lo', 'artists', 'nathan', 'sharon', 'dancer', 'dancers', 'tyra', 'ryan', 'adam', 'howie', 'daly', 'acts', 'cee', 'com', 'designer', 'auditions', 'piers', 'coaches', 'models', 'melanie', 'tim', 'fabulous', 'performing', 'talented', 'comic', 'designers', 'comedy', 'carson', 'haley', 'jazz', 'makeup', 'hip', 'lucas', 'guitar', 'öä']\n"
     ]
    }
   ],
   "source": [
    "topn = 50\n",
    "for i in range(k):\n",
    "    dist = topic_word_distrib[i]\n",
    "    ind = np.argpartition(dist, -topn)[-topn:]\n",
    "    ind = list(ind[np.argsort(dist[ind])])\n",
    "    ind.reverse()\n",
    "    print(\"cluster\", i)\n",
    "    res = [voc[i] for i in ind]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 0\n",
    "clusters = dict.fromkeys([i for i in range(k)], [])\n",
    "for doc in doc_topic_distrib:\n",
    "    c = np.argmax(doc)\n",
    "    clusters[c] = clusters[c] + [cpt]\n",
    "    cpt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    with open(\"20_clusters_voc_53540_mindf_20_max_df_0.4/cluster_\"+str(i), \"w\") as f:\n",
    "            f.write(\"Cluster numéro \"+str(i)+\" :\\n\\n\")\n",
    "            for ind in clusters[i]:\n",
    "                f.write(str(d_name[ind])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "63\n",
      "69\n",
      "taille vocabulaire :  5609\n",
      "    top1   top2     top3       top4   top5    top6     top7    top8      top9  \\\n",
      "0   nick   pete  morelli  kaczmarek   mike   nicky     zoey  dwayne      cole   \n",
      "1  garza  cyrus    tracy      beals  eddie  mereta  lucinda  kelvin  trujillo   \n",
      "\n",
      "  top10  \n",
      "0  cody  \n",
      "1  daws  \n",
      "73\n",
      "77\n",
      "taille vocabulaire :  10611\n",
      "    top1      top2    top3      top4 top5      top6  top7      top8  \\\n",
      "0  maria    monica  wright  madeline   ms    brogan  glen     peale   \n",
      "1  nucky  thompson    fuck   fucking  eli  atlantic    al  laughing   \n",
      "\n",
      "        top9     top10  \n",
      "0   sellards  santiago  \n",
      "1  schroeder    chalky  \n",
      "84\n",
      "89\n",
      "taille vocabulaire :  7397\n",
      "       top1      top2   top3    top4    top5     top6       top7   top8  \\\n",
      "0     honor    client   lily  freddy   trial  chelsea     victim   beth   \n",
      "1  growllng  creature  julie   mcgee  banner    davld  screamlng  rocky   \n",
      "\n",
      "        top9    top10  \n",
      "0  detective  rebecca  \n",
      "1      gamma   norman  \n",
      "150\n",
      "189\n",
      "taille vocabulaire :  13593\n",
      "    top1       top2    top3   top4        top5     top6   top7       top8  \\\n",
      "0  mason  defendant  murder  perry  lieutenant    della  tragg  objection   \n",
      "1     ty        lou   caleb    tim      ashley  spartan  rodeo    totally   \n",
      "\n",
      "        top9         top10  \n",
      "0      omice  fingerprints  \n",
      "1  heartland          damn  \n",
      "209\n",
      "265\n",
      "taille vocabulaire :  14657\n",
      "       top1       top2  top3      top4     top5      top6    top7  top8  \\\n",
      "0     morse  lnspector    ls    oxford    lewls       aii    weii    ln   \n",
      "1  sipowicz     simone  greg  martinez  medavoy  homicide  marino  lieu   \n",
      "\n",
      "      top9      top10  \n",
      "0   london     barrie  \n",
      "1  asshole  giardella  \n",
      "271\n",
      "288\n",
      "taille vocabulaire :  6355\n",
      "     top1     top2    top3    top4   top5       top6      top7  top8    top9  \\\n",
      "0    kate     reed  justin  lauren    leo  mediation     sighs   ben  laughs   \n",
      "1  steele  desmond   colin  potter  elena       mike  banerjee  vida    finn   \n",
      "\n",
      "      top10  \n",
      "0  chuckles  \n",
      "1     raped  \n",
      "333\n",
      "338\n",
      "taille vocabulaire :  3063\n",
      "        top1     top2       top3     top4      top5    top6     top7    top8  \\\n",
      "0  detective      zen    miletti  aurelio  ruspanti    faso    rings  heuber   \n",
      "1       kane  william  rosnovski    baron      abel  hotels  osborne   stock   \n",
      "\n",
      "     top9     top10  \n",
      "0  murder        er  \n",
      "1  lester  richland  \n",
      "382\n",
      "441\n",
      "taille vocabulaire :  8970\n",
      "    top1      top2       top3  top4      top5      top6       top7    top8  \\\n",
      "0    rex     moser  brandtner  böck    vienna  höllerer     morini  stocki   \n",
      "1  billy  costello      clive    er  chambers      silk  defendant    sean   \n",
      "\n",
      "     top9       top10  \n",
      "0  search  stockinger  \n",
      "1    jake       clerk  \n",
      "444\n",
      "446\n",
      "taille vocabulaire :  5750\n",
      "     top1     top2        top3   top4      top5  top6   top7    top8     top9  \\\n",
      "0   oscar     nora      thomas   boat  sandhamn  lina  jonny  henrik  krister   \n",
      "1  brogan  haldane  lieutenant  gonna     slomo   sir   matt  castle       ya   \n",
      "\n",
      "   top10  \n",
      "0  göran  \n",
      "1     um  \n",
      "473\n",
      "527\n",
      "taille vocabulaire :  2330\n",
      "      top1    top2      top3        top4    top5    top6    top7    top8  \\\n",
      "0   snorri   cliff  accident  explosives    anna    inga   helgi  veigar   \n",
      "1  hjalmar  bjarne       ole      madsen  arthur  zemich  volmer  gordan   \n",
      "\n",
      "    top9    top10  \n",
      "0  larus    lárus  \n",
      "1  olsen  lisbeth  \n",
      "560\n",
      "576\n",
      "taille vocabulaire :  12014\n",
      "         top1   top2   top3  top4       top5     top6    top7     top8  top9  \\\n",
      "0  montalbano  fazio  salvo  mimi  catarella       ls  vigata  augello    di   \n",
      "1       jared  peter  honor  karp     pindar  stanton  infeld   carmen  bash   \n",
      "\n",
      "   top10  \n",
      "0  livia  \n",
      "1  hanna  \n",
      "586\n",
      "595\n",
      "taille vocabulaire :  11739\n",
      "      top1   top2        top3   top4    top5       top6        top7     top8  \\\n",
      "0  columbo   tape  chattering     lt    alex  chuckling  williamson  angeles   \n",
      "1   ellery  velie   flannigan  winer  mckell  wentworth      mcgraw    nivin   \n",
      "\n",
      "     top9    top10  \n",
      "0     los   harold  \n",
      "1  packer  manning  \n",
      "600\n",
      "637\n",
      "taille vocabulaire :  10139\n",
      "     top1   top2    top3     top4     top5    top6     top7      top8  \\\n",
      "0   riina   toto  biagio   liggio  falcone  schirn    totã²   bontade   \n",
      "1  harvey  louis    mike  jessica   rachel   wanna  partner  actually   \n",
      "\n",
      "      top9    top10  \n",
      "0  ninetta    binnu  \n",
      "1  hardman  specter  \n",
      "654\n",
      "678\n",
      "taille vocabulaire :  7606\n",
      "     top1      top2    top3    top4          top5    top6        top7  \\\n",
      "0    nick        lt  fallin  father          talk     son         dad   \n",
      "1  mickey  hamilton   peter     ash  surveillance  artist  investment   \n",
      "\n",
      "      top8    top9 top10  \n",
      "0    alvin  burton    um  \n",
      "1  redford  lesson   org  \n",
      "706\n",
      "714\n",
      "taille vocabulaire :  2618\n",
      "    top1    top2   top3       top4     top5      top6    top7      top8  \\\n",
      "0  bunny  dalton   nick      night  maureen      girl   lynne     carol   \n",
      "1  leigh   piece  value  rembrandt     keno  original  violin  treasure   \n",
      "\n",
      "      top9    top10  \n",
      "0  playboy    bruno  \n",
      "1  million  century  \n",
      "716\n",
      "751\n",
      "taille vocabulaire :  10078\n",
      "      top1      top2    top3   top4  top5    top6    top7    top8  \\\n",
      "0    jimmy      beth  becker   dave  joey  johnny  coombs   gandy   \n",
      "1  kolchak  vincenzo     ron  emily    ng      ri  morton  palmer   \n",
      "\n",
      "           top9   top10  \n",
      "0        martin   terry  \n",
      "1  matchemonedo  ripper  \n",
      "757\n",
      "768\n",
      "taille vocabulaire :  4491\n",
      "     top1    top2   top3     top4   top5   top6        top7    top8     top9  \\\n",
      "0     guy   thank  wanna      car  stone   farr        hold   steve  minutes   \n",
      "1  freddy  kruger   gene  merritt  sarah  deets  springwood  honour     feds   \n",
      "\n",
      "  top10  \n",
      "0   day  \n",
      "1   agh  \n",
      "777\n",
      "795\n",
      "taille vocabulaire :  8797\n",
      "         top1    top2   top3     top4     top5     top6    top7      top8  \\\n",
      "0          er  dwayne  fidel  camille      erm  carlton  stevie    talbot   \n",
      "1  lieutenant   kojak  wanna  crocker  stavros    hecht   artie  donnelly   \n",
      "\n",
      "    top9     top10  \n",
      "0     hm     jenny  \n",
      "1  march  giordino  \n",
      "834\n",
      "861\n",
      "taille vocabulaire :  12791\n",
      "      top1     top2    top3     top4     top5    top6     top7    top8   top9  \\\n",
      "0  rebecca    holly  rashid  theresa  foreman  cheung      mum  jurors  tahir   \n",
      "1      jim  colonel  barney    major  country   frank  captain   willy   plan   \n",
      "\n",
      "      top10  \n",
      "0  internet  \n",
      "1      wife  \n",
      "872\n",
      "889\n",
      "taille vocabulaire :  8860\n",
      "    top1    top2       top3     top4        top5        top6    top7  \\\n",
      "0   fuck  lawyer       cops  ronaldo          ga  prosecutor  tintin   \n",
      "1  morse  oxford  constable     mary  cartwright       frida  county   \n",
      "\n",
      "       top8      top9  top10  \n",
      "0     roban    pierre  gilou  \n",
      "1  tremlett  pettifer     em  \n",
      "890\n",
      "939\n",
      "taille vocabulaire :  5997\n",
      "      top1    top2   top3    top4    top5    top6    top7    top8     top9  \\\n",
      "0  mcdeere   mitch   abby  honour  claire  andrew   noble   moxon    tammy   \n",
      "1     jake  amelia  aster    lucy    bohm   avram  teller  trevor  norburg   \n",
      "\n",
      "       top10  \n",
      "0  defendant  \n",
      "1  mccormick  \n",
      "947\n",
      "959\n",
      "taille vocabulaire :  9036\n",
      "     top1   top2       top3    top4     top5     top6   top7       top8  \\\n",
      "0   caleb  lucas    sheriff    gail   merlyn    emory  daddy      merly   \n",
      "1  fisher   jack  inspector  phryne  collins  alright   hugh  constable   \n",
      "\n",
      "     top9   top10  \n",
      "0  temple  crower  \n",
      "1    jane  butler  \n",
      "969\n",
      "1021\n",
      "taille vocabulaire :  3517\n",
      "      top1  top2    top3     top4    top5        top6     top7    top8   top9  \\\n",
      "0  macduff  dirk  gently  edwards  gordon       susan  jericho     cat  henry   \n",
      "1    frank    lt   gonna    eddie   sally  lieutenant   drebin  johnny  terri   \n",
      "\n",
      "       top10  \n",
      "0  horoscope  \n",
      "1      buddy  \n",
      "1068\n",
      "1070\n",
      "taille vocabulaire :  4740\n",
      "      top1    top2   top3    top4      top5  top6   top7    top8     top9  \\\n",
      "0  randall  lizzie    eva  franny      egan   tom     ed   ricky  kathryn   \n",
      "1    nicky    cole  matty   errol  sergeant  rick  yates  claire      mum   \n",
      "\n",
      "     top10  \n",
      "0  malcolm  \n",
      "1    paddy  \n",
      "1080\n",
      "1164\n",
      "taille vocabulaire :  9145\n",
      "     top1   top2      top3    top4    top5      top6    top7   top8     top9  \\\n",
      "0     amy  bleep  children  shadow  sketch  basement    land  crazy   father   \n",
      "1  denton  gates        ds  dryden  ambush     akers  arnott     dc  lindsay   \n",
      "\n",
      "         top10  \n",
      "0  experiences  \n",
      "1          dci  \n",
      "1191\n",
      "1197\n",
      "taille vocabulaire :  2796\n",
      "    top1   top2      top3      top4     top5    top6    top7    top8  \\\n",
      "0    sir  harry  crawford  sergeant      car   oscar   night    dock   \n",
      "1  brady    ian  ashworth     keith  hindley  winnie  mental  hunger   \n",
      "\n",
      "        top9    top10  \n",
      "0  inspector     andy  \n",
      "1     battle  bennett  \n",
      "1254\n",
      "1273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille vocabulaire :  4202\n",
      "    top1     top2     top3    top4     top5    top6     top7    top8  \\\n",
      "0  renzo    laura  jiménez   bitch  feldman  marina  benitez  martin   \n",
      "1  honor  martina      tim  leland    mason      ms     emma    doug   \n",
      "\n",
      "       top9  top10  \n",
      "0     bruno   nico  \n",
      "1  garretti  cause  \n",
      "1282\n",
      "1347\n",
      "taille vocabulaire :  2964\n",
      "    top1    top2        top3     top4    top5   top6   top7    top8   top9  \\\n",
      "0  roger  edward   sebastian    vanja  ragnar    leo  johan  torkel  motel   \n",
      "1   zach   ethan  canterbury  russell     liz  trial  honor  jasper   jury   \n",
      "\n",
      "   top10  \n",
      "0  hinde  \n",
      "1  louis  \n",
      "1353\n",
      "1372\n",
      "taille vocabulaire :  3700\n",
      "      top1   top2    top3   top4   top5     top6    top7    top8      top9  \\\n",
      "0      bob   iris  hannah  kevin  rajeb  charles     tim      00  lucienne   \n",
      "1  camille  simon    léna  adèle    dam     toni  audrey  victor     gonna   \n",
      "\n",
      "   top10  \n",
      "0  lotte  \n",
      "1   town  \n",
      "1417\n",
      "1439\n",
      "taille vocabulaire :  4083\n",
      "     top1    top2    top3      top4     top5      top6  top7      top8 top9  \\\n",
      "0  vivian  bowers  julian    edward      mia    joanna   dad  lyritrol  ben   \n",
      "1     mma     rra  dumela  ramotswe  michael  botswana  note      pula  tea   \n",
      "\n",
      "      top10  \n",
      "0     wanna  \n",
      "1  matekoni  \n",
      "1525\n",
      "1526\n",
      "taille vocabulaire :  16045\n",
      "        top1    top2      top3       top4     top5   top6    top7       top8  \\\n",
      "0  detective   frank  homicide  baltimore  bayliss  munch   lewis  kellerman   \n",
      "1      norma  romero   summers     annika     cody    gil  blaire      caleb   \n",
      "\n",
      "   top9       top10  \n",
      "0   cos   pembleton  \n",
      "1  zane  madhatters  \n",
      "1575\n",
      "1607\n",
      "taille vocabulaire :  4431\n",
      "    top1      top2    top3    top4    top5     top6       top7      top8  \\\n",
      "0    sir  sergeant  dawson  fenton   sarge  william  inspector  brinford   \n",
      "1  dicte    wagner   badul   lotte  nicole    peter        mom    jannik   \n",
      "\n",
      "       top9    top10  \n",
      "0    briggs   powell  \n",
      "1  frederik  tiffany  \n",
      "1611\n",
      "1615\n",
      "taille vocabulaire :  2788\n",
      "      top1   top2      top3   top4      top5    top6    top7      top8  top9  \\\n",
      "0     nina    van     vliet  phone  daughter  legoff     car    barron    jo   \n",
      "1  ziegler  molly  hastings  terry  rosanski  cooper  tongue  swastika  mary   \n",
      "\n",
      "     top10  \n",
      "0    peter  \n",
      "1  collins  \n",
      "1636\n",
      "1646\n",
      "taille vocabulaire :  1879\n",
      "       top1   top2      top3  top4    top5    top6  top7  top8    top9 top10\n",
      "0   holland    amy      reid   ray  denise     dna  mike   van  lauren   sir\n",
      "1  clarence  maria  rosmarie  disa     odd  marcus  ivan  loki    dick  haag\n",
      "1654\n",
      "1657\n",
      "taille vocabulaire :  2728\n",
      "       top1   top2  top3       top4   top5   top6    top7   top8  top9  \\\n",
      "0  brooklyn  danny   art  procedure  pride   isel      dr  davey  jury   \n",
      "1      kurt  tommy  ella  wallander  helen  linda  salino  sorry  erik   \n",
      "\n",
      "      top10  \n",
      "0  painting  \n",
      "1     julia  \n",
      "1738\n",
      "1748\n",
      "taille vocabulaire :  3447\n",
      "       top1   top2     top3      top4       top5        top6   top7  \\\n",
      "0  jonkhere   paul  gerardi  minister  rasenberg  salamander  sofie   \n",
      "1     gonna  frank  brendan    mccann      billy         ass    joe   \n",
      "\n",
      "        top8     top9    top10  \n",
      "0     bloody  darling  laridon  \n",
      "1  detective    agnew    damon  \n",
      "1753\n",
      "1814\n",
      "taille vocabulaire :  3583\n",
      "           top1    top2        top3   top4       top5       top6     top7  \\\n",
      "0          knox  rourke       danny   york      ellis      bryce  charlie   \n",
      "1  commissioner   fazio  montalbano  hello  catarella  inspector   vigata   \n",
      "\n",
      "      top8   top9  top10  \n",
      "0  marsden   sean   jess  \n",
      "1    house  salvo  dindo  \n",
      "1817\n",
      "1841\n",
      "taille vocabulaire :  10466\n",
      "     top1       top2      top3     top4      top5      top6     top7    top8  \\\n",
      "0  murder     client     court    james    ronnie     trial   charge    mate   \n",
      "1    otto  enderbury  truscott  jeannie  mulligan  margaret  dearest  cyprus   \n",
      "\n",
      "    top9   top10  \n",
      "0   kids      ds  \n",
      "1  mehta  ronald  \n",
      "1880\n",
      "1886\n",
      "taille vocabulaire :  3116\n",
      "    top1    top2    top3    top4 top5 top6  top7     top8   top9 top10\n",
      "0  foyle  burton   jamie    kate  dna  dad  lord  defence   liam  jury\n",
      "1  hecky   gonna  mickey  siegel  joe  cop   sid     nash  cohen  guys\n",
      "1909\n",
      "1960\n",
      "taille vocabulaire :  3576\n",
      "    top1      top2    top3        top4         top5      top6 top7   top8  \\\n",
      "0  lucan  veronica  aspers      sandra         john  dominick  ian  jimmy   \n",
      "1  bjorn    willem     dad  couwenberg  steenhouwer      iris   eh  elsie   \n",
      "\n",
      "     top9    top10  \n",
      "0   susie  pearson  \n",
      "1  lawyer    louis  \n",
      "1979\n",
      "1997\n",
      "taille vocabulaire :  6892\n",
      "        top1     top2     top3      top4      top5    top6    top7  top8  \\\n",
      "0      deane      kee   keegan   scarlet       ben  leanne  gloria  finn   \n",
      "1  kellerman  kessler  charlie  michelle  ernhardt   bobbi    nick   roz   \n",
      "\n",
      "     top9    top10  \n",
      "0  potter    mikki  \n",
      "1   balco  richard  \n",
      "2089\n",
      "2148\n",
      "taille vocabulaire :  9311\n",
      "    top1       top2   top3    top4       top5       top6    top7    top8  \\\n",
      "0  stark  sebastian   cops  victim      julie     taylor   wayne   isaac   \n",
      "1   rita     postal  toole   shane  mcinerney  mclnerney  phoebe  denver   \n",
      "\n",
      "       top9      top10  \n",
      "0  callison  defendant  \n",
      "1    amidon    pageant  \n",
      "2158\n",
      "2172\n",
      "taille vocabulaire :  5735\n",
      "    top1     top2    top3    top4        top5        top6       top7    top8  \\\n",
      "0    lee     anne     roy  marcus  charleston  mccandless     sawyer  arliss   \n",
      "1  erich  strauss  dustin   kaleb        shit      neyers  inspector   maker   \n",
      "\n",
      "      top9  top10  \n",
      "0  preston   knox  \n",
      "1    hildy  kevin  \n",
      "2200\n",
      "2201\n",
      "taille vocabulaire :  2739\n",
      "      top1       top2   top3   top4  top5   top6       top7   top8   top9  \\\n",
      "0    terry  bankowski    dna  jared  zale  kucik  christine  jenny  clark   \n",
      "1  camille       lena  simon  rowan  tony   jack      tommy   lucy    ben   \n",
      "\n",
      "    top10  \n",
      "0  butler  \n",
      "1   julie  \n",
      "2213\n",
      "2220\n",
      "taille vocabulaire :  6070\n",
      "     top1     top2      top3   top4       top5    top6    top7     top8  \\\n",
      "0     amy  madison  anderson   rose  cranfield    gary    shit   whelan   \n",
      "1  gotham   gordon   falcone  bruce      wayne  maroni  alfred  barbara   \n",
      "\n",
      "      top9    top10  \n",
      "0     todd  seattle  \n",
      "1  penguin   arkham  \n",
      "2250\n",
      "2254\n",
      "taille vocabulaire :  3729\n",
      "      top1     top2      top3    top4     top5     top6   top7 top8    top9  \\\n",
      "0  georgie    poppy  narrator  lovely  carlton  british  royal  jay   derek   \n",
      "1    eddie  everest   seymour   poker  matador    cards  towne   em  casino   \n",
      "\n",
      "      top10  \n",
      "0  american  \n",
      "1     clark  \n",
      "2255\n",
      "2297\n",
      "taille vocabulaire :  2446\n",
      "         top1     top2    top3      top4    top5    top6       top7    top8  \\\n",
      "0      guilty  malcolm   gonna  governor   night   danny     miller    anna   \n",
      "1  undercover     anne  gambon   dimitri  walter  isaacs  policeman  romney   \n",
      "\n",
      "    top9      top10  \n",
      "0   shit  pritchard  \n",
      "1  greek      chief  \n",
      "2428\n",
      "2444\n",
      "taille vocabulaire :  5520\n",
      "     top1    top2  top3     top4   top5     top6     top7   top8       top9  \\\n",
      "0  rafael   petra   mom  rogelio  hotel  xiomara    luisa    sin     rostro   \n",
      "1     lew  barker  rita    simon  brian    wally  staziak  wanna  helverson   \n",
      "\n",
      "      top10  \n",
      "0      rose  \n",
      "1  transcor  \n",
      "2483\n",
      "2508\n",
      "taille vocabulaire :  3215\n",
      "     top1 top2    top3   top4       top5    top6  top7    top8   top9  \\\n",
      "0  ghitta  rom  soneri   emma  inspector    chie   ind  bondan  riend   \n",
      "1   durst  bob  kathie  susan     robert  morris  york  family     um   \n",
      "\n",
      "       top10  \n",
      "0        wie  \n",
      "1  galveston  \n",
      "2510\n",
      "2576\n",
      "taille vocabulaire :  2335\n",
      "     top1      top2   top3      top4       top5    top6      top7      top8  \\\n",
      "0     eva  josefine  anton   nicklas        tom    hell  children      emma   \n",
      "1  harold    goldie  lilac  dinosaur  squirrels  crayon    purple  realized   \n",
      "\n",
      "      top9     top10  \n",
      "0   police       mom  \n",
      "1  mermaid  birthday  \n",
      "2608\n",
      "2640\n",
      "taille vocabulaire :  4842\n",
      "     top1     top2     top3     top4     top5       top6  top7    top8  \\\n",
      "0  lizzie     emma   borden  siringo    adele    spencer  almy  leslie   \n",
      "1    jeff  randall  jeannie    marty  hopkirk  inspector    er   phone   \n",
      "\n",
      "        top9   top10  \n",
      "0  cavanaugh     god  \n",
      "1       jean  london  \n",
      "2721\n",
      "2748\n",
      "taille vocabulaire :  3830\n",
      "      top1      top2   top3  top4      top5      top6      top7      top8  \\\n",
      "0  luciano  genovese  dewey  vito  costello     mafia  masseria    siegel   \n",
      "1  housing   yonkers   nick  fuck   fucking  wasicsko   zaleski  spallone   \n",
      "\n",
      "     top9     top10  \n",
      "0  lansky  narrator  \n",
      "1   votes     units  \n",
      "2773\n",
      "2886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taille vocabulaire :  16432\n",
      "      top1      top2       top3   top4      top5  top6         top7  \\\n",
      "0  murdoch  crabtree  constable  ogden  pendrick  sirs  brackenreid   \n",
      "1  grayson     stacy     bingum    kim       deb  teri      kaswell   \n",
      "\n",
      "           top8     top9    top10  \n",
      "0  constabulary  garland  gillies  \n",
      "1          sync    super    bobbi  \n",
      "2896\n",
      "2911\n",
      "taille vocabulaire :  5230\n",
      "      top1   top2      top3       top4   top5     top6        top7    top8  \\\n",
      "0  matlock  tyler  charlene  defendant  baron  edwards     walters  harold   \n",
      "1    estep   rudy       wes   jordache  billy  charles  falconetti   diane   \n",
      "\n",
      "    top9       top10  \n",
      "0    les        tate  \n",
      "1  wasrt  washington  \n",
      "2920\n",
      "2951\n",
      "taille vocabulaire :  15845\n",
      "     top1      top2   top3     top4     top5     top6      top7    top8  \\\n",
      "0   kibre      soin  lucas  lascher  voychek   peluso    duvall   colby   \n",
      "1  alicia  florrick   cary      eli  kalinda  gardner  lockhart  bishop   \n",
      "\n",
      "        top9  top10  \n",
      "0  hernandez  petro  \n",
      "1      david   agos  \n",
      "2952\n",
      "2972\n",
      "taille vocabulaire :  9231\n",
      "       top1     top2    top3       top4    top5    top6   top7    top8  \\\n",
      "0       ron     beth  luther      karen  mortin   stein  trott  regina   \n",
      "1  annabeth  maureen  murphy  schraeder  veeder  brooks  billy    eric   \n",
      "\n",
      "      top9   top10  \n",
      "0     jake  claire  \n",
      "1  hellman  conlon  \n",
      "2976\n",
      "2982\n",
      "taille vocabulaire :  11789\n",
      "          top1     top2     top3    top4          top5      top6     top7  \\\n",
      "0        sarge     mate     matt  duncan         simon  jennifer   bloody   \n",
      "1  diefenbaker  vecchio  mountie    dief  neighborhood     benny  chicago   \n",
      "\n",
      "       top8    top9    top10  \n",
      "0   stanley    nick    allie  \n",
      "1  canadian  willie  caribou  \n",
      "2983\n",
      "3022\n",
      "taille vocabulaire :  26247\n",
      "      top1   top2    top3  top4       top5    top6      top7       top8  \\\n",
      "0   kimble     iâ   elvie  lars     herbie  hallie  josephus      hanes   \n",
      "1  briscoe  mccoy  lennie  adam  prosecute     sex      rape  attorneys   \n",
      "\n",
      "    top9      top10  \n",
      "0   vale     briggs  \n",
      "1  bobby  offenders  \n",
      "3029\n",
      "3107\n",
      "taille vocabulaire :  9188\n",
      "    top1   top2  top3    top4     top5   top6    top7   top8      top9   top10\n",
      "0   jake    des   mal  leslie     rose  gotta   tinny  doyle   alright      ya\n",
      "1  cliff  susan  hart  hewitt  malcolm    mei  hannah  rowdy  branford  mather\n",
      "3119\n",
      "3120\n",
      "taille vocabulaire :  13372\n",
      "   top1    top2       top3      top4     top5    top6   top7   top8    top9  \\\n",
      "0  kidd  honour  hennessey  manoulis      mal  glenda   shit  shirl  kirsty   \n",
      "1  monk  adrian    captain   natalie  sharona   randy  trudy    com    nate   \n",
      "\n",
      "      top10  \n",
      "0  grossman  \n",
      "1        dr  \n",
      "3127\n",
      "3173\n",
      "taille vocabulaire :  17192\n",
      "        top1     top2     top3      top4     top5    top6  top7     top8  \\\n",
      "0   fletcher  jessica  sheriff      jess     seth   cabot  amos    grady   \n",
      "1  mcgarrett    danno     kono  nordhoff  waldron  vashon  fong  kalakua   \n",
      "\n",
      "      top9    top10  \n",
      "0  hazlitt  metzger  \n",
      "1    shang  rolande  \n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "nlargest = 10\n",
    "cpt = 0\n",
    "for ind in clusters[7]:\n",
    "    print(ind)\n",
    "    if cpt %2== 0 and cpt > 0:\n",
    "        \n",
    "        sparsemat, df = getTfidfSparseMatAndDataFrame(c, my_stopwords=stopwords_set, max_df=0.8)\n",
    "        c = []\n",
    "        nlargest = 10\n",
    "        order = np.argsort(-df.values, axis=1)[:, :nlargest]\n",
    "        result = pd.DataFrame(df.columns[order], \n",
    "                              columns=['top{}'.format(i) for i in range(1, nlargest+1)],\n",
    "                              index=df.index)\n",
    "        print(result)\n",
    "    \n",
    "    \n",
    "    cpt += 1\n",
    "    c.append(corpus[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-700512248846>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0msparsemat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTfidfSparseMatAndDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/users/nfs/Etu8/3416148/PLDAC_Recommandation_analyse_sous_titres-master/utils/preprocessing.py\u001b[0m in \u001b[0;36mgetTfidfSparseMatAndDataFrame\u001b[0;34m(corpus, my_stopwords, my_tokenizer, max_features, min_df, max_df)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetTfidfSparseMatAndDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taille vocabulaire : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "c = []\n",
    "cpt = 0\n",
    "dico_list = []\n",
    "nb_series = len(clusters[7])\n",
    "nlargest = 10\n",
    "\n",
    "for i in range(0, nb_series):\n",
    "    #if i != 2:\n",
    "        #break\n",
    "    c = []\n",
    "    c.append(corpus[clusters[7][i]])\n",
    "    dico = dict()\n",
    "    #print(i)\n",
    "    for j in range(0, nb_series):\n",
    "        #print(i, j)\n",
    "        if i != j :\n",
    "            c.append(corpus[clusters[7][j]])\n",
    "            \n",
    "            sparsemat, df = getTfidfSparseMatAndDataFrame(c, my_stopwords=stopwords_set, max_df = 0.9)\n",
    "            order = np.argsort(-df.values, axis=1)[:, :nlargest]\n",
    "            \n",
    "            \n",
    "            result = pd.DataFrame(df.columns[order], \n",
    "                              columns=['top{}'.format(k) for k in range(1, nlargest+1)],\n",
    "                              index=df.index)\n",
    "            \n",
    "            top_tfidf = list(result.values.tolist()[0])\n",
    "            #print(top_tfidf)\n",
    "            for word in top_tfidf:\n",
    "                if word not in dico.keys():\n",
    "                    dico[word] = 1\n",
    "                else:\n",
    "                    dico[word] += 1\n",
    "            c.pop()\n",
    "    dico_list.append(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = []\n",
    "for i in range(len(dico_list)):\n",
    "    l = []\n",
    "    sorted_x = sorted(dico_list[i].items(), key=lambda kv: kv[1])\n",
    "    for t in sorted_x:\n",
    "        if t[1] > seuil:\n",
    "            l.append(t)\n",
    "    words_list.append(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
